#import "includes/template.typ": bachelor_conf, seu_bibliography
#import "includes/template.typ": acknowledgment, appendix
#import "includes/utils.typ": set_doc_footnote, subfigure
#import "@preview/funarray:0.3.0":cycle
#import "@preview/tablex:0.0.8": tablex,vlinex,hlinex,colspanx,rowspanx

#show :doc => set_doc_footnote(doc)


#let info = (
  title: [面向低分辨率场景文本图像的增强与识别技术研究],
  student_id: [61520324],
  name: [许睿],
  college: [吴健雄学院],
  major: [计算机科学与技术],
  supervisor: [薛晖],
  duration: "2023.12~2024.6",

  zh_abstract: [
    中文摘要
  ],

  zh_key_words: (
    "关键字1",
    "关键字2"
  ),

  en_abstract: [
    #lorem(250)
  ],

  en_key_words: (
    "Keywords1",
    "Keywords2"
  )
)


#show :doc => bachelor_conf(
  doc,
  ..info
)

#figure(
  tablex(
    auto-lines: false,
    columns: cycle((1fr, 2fr, 1fr), 3),
    stroke: 1pt,
    align: left+horizon,

    hlinex(),
    [术语], [英文], [中文],
    hlinex(),
    [STR], [Scene-Text Recognition], [场景文本识别],
    [STISR], [Scene-Text Image Super-Resolution], [场景文本图片超分],
    [NLP], [Natural Language Processing], [自然语言处理],
    [OCR], [Optical Character Recognition], [光学字符识别],
    [LR], [Low Resolution], [低分辨率],
    [HR], [High Resolution], [高分辨率],
    [SR], [Super-Resolution], [超分辨率],
    [PSNR], [Peak Signal Noise Ratio], [峰值信噪比],
    [SSIM], [Structural Similarity Index Measure], [结构一致性指标],
    [SRB], [Sequential Residual Block], [序列残差块],
    [SOTA], [State of the arts], [现有的工作],
    [ACC], [Accuracy], [准确率],
    [FLOPs], [Floating-point Operations], [浮点运算数],
    [Params],[Parameters], [参数量],
    [FPS], [Frequence per Second], [每秒推理图片张数],
    hlinex()
  ),
  kind: table,
  caption: [中英术语对照表]
)

= 绪论

== 课题背景和意义

//TODO: 补充图片，说明SITSR, STR
// draw image:
// LR image
// SR image
// HR image
从一般性的场景图片中识别文本信息不仅能帮助深度学习模型在训练时理解场景逻辑，还能在推理时给予使用者更多场景相关的信息，该类型的任务被称为场景文本识别（Scene-Text Recognition, STR）。
场景文本识别能在诸多实际应用中有重要意义：例如在辅助驾驶中，汽车可以动态识别路上的交通符号，以便根据路况和交通规则做出安全的决策。
近年来有许多研究人员躬身场景文本识别领域并取得了很多优秀成果。这些模型在目前广泛使用的基准数据集 ~@cute80~@icdar2013~@coco-text 中对于高分辨率（High-Resolution, HR）的图像取得了非常高的准确率。
现实中的STR任务比基准数据集的形式更为复杂，被识别的字形可能因设计要求等因素，单个字符可以有多种字体和不同程度的扭曲，因此也有很多研究人员提出可以使用矫正模块 @aster 对场景文本进行空间上的矫正，使其便于识别。因此给一张比较清晰的场景文本图像，当前有许多方式可以以较高置信度对其进行识别。

然而，在场景文本图像中的特征受到大量影响时，识别器的性能也大幅下降。例如，受到拍摄环境或拍摄器材的制约时，拍摄的场景文本图像可能带有一定的模糊，这对识别器的鲁棒性带来了巨大的挑战。
因此有研究人员 @tsrn 提出了更加一般的STR任务，即对于带有模糊等不良因素的低分辨率（Low-Resolution, LR）图像，可以先进行场景文本图像超分辨率（Scene-Text Image Super-Resolution, STISR）处理，得到较为清晰的超分辨率（Super-Resolution, SR）图像后，再进行常规的STR。该研究人员构建了专门的模糊场景文本数据集 @tsrn 用于评判模型在不良条件下的超分辨率和识别能力。与传统的图像超分辨率（Image Super-Resolution）任务不同，STISR任务更加注重于文本的恢复，由于不少文本包含一定的语义信息，这不仅给图像超分辨率模型带来了挑战，也使STISR模型得到更多新的思路。

对于LR场景文本的识别，当前广泛使用的模型架构是如@fig1-1a 所示的串行架构。这种架构虽然直观，但同时存在不少问题。串行架构中，两个任务的耦合程度太高，STR任务非常依赖于STISR任务的性能，如果STISR得到的SR场景文本图像存在一定的问题，则会对识别任务造成很大的麻烦；同时，STISR任务模型的监督信号含有一定的识别损失而不是专门的超分辨率损失，因此并不会生成比较完整的SR场景文本图像。而本文实现的模型是如@fig1-1b 所示的并行架构，该架构首先使用统一的编码器将输入的图片映射到一个统一的特征空间，再通过不同任务的解码器输出相应的预测目标。该架构的优势在于多种任务使用之间的耦合度低，识别效果受到SR质量影响的程度较低，同时SR的质量也不受识别损失的干扰。

#figure(
  grid(
    columns: 2,
    gutter: 5%,
    [#subfigure(
      image("fig/fig1-1a.svg"),
      caption: [STISR和STR模型的串行架构]
    )<fig1-1a>],
    [#subfigure(
      image("fig/fig1-1b.svg"),
      caption: [STISR和STR模型的并行架构]
    )<fig1-1b>],
  ),
  caption: [两种STISR和STR结合的架构]
)<fig1-1>

== 本文研究内容

本文主要使用如@fig1-1b 所示的多任务模型架构，分别设计了特征编码器、特征增强模块、识别解码器和超分辨率解码器四个部分，并使用了两阶段训练和多任务损失函数进行模型监督训练，最终实现了从LR场景文本图像到文本内容和SR图像的端到端任务。本文的主要贡献如下：

+ 提出了一般性的LR场景文本图像识别和超分辨率的模型架构；
+ 本文的模型使用了基于隐式扩散模型的特征增强模块，并在推理时对扩散模型进行有效的控制，实现在特征层面上增强LR图像；
+ 模型采用两阶段的训练模型，在第一阶段进行预训练，目的是让模型保存高分辨率特征信息，在第二阶段进行特征增强，目的是提升LR图像特征信息，提升多任务模型的泛化能力；
+ 本文使用基于不确定性权重的损失函数进行多任务模型训练，实现STR任务和STISR任务之间的平衡；

== 论文各章节安排

本文主要讨论STR任务和STISR任务的多任务模型和特征增强方法，以及本文所实现的模型在数据集上的性能。
第二章介绍了目前现有的场景文本识别模型和场景文本图像超分辨率模型以及用于特征增强的扩散模型。
第三章将会着重介绍本文提出的模型架构、模型设计思路的实现方式，并会从各个模块出发，分别介绍该模型实现特征增强的方式和多任务解码方式。
第四章主要评估第三章中模型的性能，并展示了多种对比结果。
最后，第五章将会对本文提出的方法进行讨论和总结。

= 相关工作

本章将分别介绍场景文本识别（STR）模型和场景文本图像超分辨率（STISR）模型，并讨论其与本文模型的关系。除此之外，本章还将介绍本工作将要使用的TextZoom数据集，以便后文能顺利地引入本文模型。

== 场景文本识别

// * NOTES: cnn/rnn-based, attention-based, large model-based
// 1. cnn/rnn: crnn, aster,
// 2. attention: most popular, parseq, svtr, CCD, , SIGA, DPAN, MGP
// 3. large model: ABINet, CLIP4STR, DTrOCR, MATRN
// ! TODO: add formulas

#figure(
  image("fig/recognizer_arch.svg", width: 75%),
  // placement: auto,
  caption: [STR识别器基本架构。其中虚线表示可选项]
)<fig2-rec>

与传统的光学字符识别（OCR）不同，场景文本识别（STR）主要针对场景中的字符进行识别，OCR面向的字符常常比较规整，而STR的识别对象经常由于拍摄问题，出现不同程度的透视或光影效果，有的甚至因为聚焦问题而出现低分辨率（LR）图像。对此问题，目前有很多研究人员提出了许多有效的方法，对高分辨率的场景文本进行识别，大部分模型的基本架构如@fig2-rec 所示，它们通常包含图像编码器用于将图像特征嵌入模型，再通过一系列的图像特征学习实现图像特征和文本特征的映射，最后使用激活函数将特征映射为标签，再经过损失函数将损失传播到模型参数，实现模型的训练。目前有研究人员在编码器和特征学习时使用卷积、循环神经网络以及注意力机制以便提取出更好的特征，除此之外，还有研究人员使用预训练的大语言模型对输出结果进行优化。
基于相关工作中使用的不同特征提取方式，本节从基于卷积和循环网络、基于注意力机制和基于大语言模型三个角度介绍目前常用的STR模型，以及他们在基准数据集上的性能。

*卷积和循环网络*。卷积和循环网络在计算时拥有绝佳的性能，用该模块设计的网络常常具有训练和推理速度快、模型体积小等特点，因此仍有大量研究人员使用其作为图片特征提取和文本信息处理的主要模块。
例如SVTR~@svtr 模型使用不同步长的卷积运算模拟ViT~@vit 将图片分割为多个小块（patch）的操作，再对其进行后续处理，实验证明，这种图片处理方式能较为高效地将图片嵌入模型。CRNN~@crnn 模型则使用多个卷积层深度提取图像特征，并使用双向的LSTM（Bidirectional LSTM, BiLSTM）~@lstm 作为图片和文本之间的映射模块，最终使用CTC损失~@ctc 监督模型训练，其识别结果在当时取得了较高性能。而对于含有透视、扭曲结构等场景文本中，ASTER~@aster 模型在其主干网络之前添加了矫正模块，将弯曲文本矫正为水平排布的文本，并最终使用基于循环网络LSTM~@lstm 和GRU~@gru 的识别模块，最终在弯曲文本的识别中取得非常好的性能。
目前，大部分的模型都使用卷积提取图片特征，这种提取方式的优势在于能够尽量保存原有的图像特征，对于文本识别有很大帮助。本文实现的模型同样使用卷积作为编码器的主干。

*注意力网络*。注意力机制在NLP领域中得到了广泛应用，使用注意力机制能大幅提升模型对较长序列处理能力，不会出现类似LSTM等RNN模型在使用较长序列进行训练时出现的梯度消失或梯度爆炸的现象。对于STR任务，存在两种使用注意力机制的情况，一是针对文本进行注意力学习，再与图像特征进行交叉注意力学习，二是先将图像的特征提取后嵌入模型，再直接对图像进行注意力学习，最后用损失约束输出结果。
例如PARSeq~@parseq 使用了多种注意力机制进行训练。该模型的创新点在于使用排列数作为注意力机制的掩码，在文本特征层面使用了自注意力机制，并与图像特征共同使用交叉注意力，学习文本特征和图像特征中的映射关系，在当时的数据集上取得了较高性能。由于PARSeq模型在多个合成数据集上进行了大规模预训练，因此最终得到的网络具有很强的先验知识，可以根据预训练网络进行微调。
前文提到SVTR~@svtr 模型利用带有不同步长的卷积运算模拟ViT将图像嵌入网络，同时，该模型还使用多个注意力机制用于提取局部和全局特征，该注意力机制使用不同大小的掩码块引导模型对图像注意力的学习。
除此之外，MGP-STR~@mgp-str 以ViT为基础架构，将输入的文本图像分割为互不重叠小块。相较于ViT将不同的图像小块（即每个块的特征）聚合在一起作为整个图像的特征表示，在图像文本识别钟，MGP-STR使用了一种基于注意力机制模块对识别器进行多重约束，用于提取更加注重于文本的特征，将图像的小块有意义地聚合在一起，使得该模型可以预测字母、词组和字母个数等不同粒度的文本信息。
DPAN~@dpan 模型在架构上进行了研究，并基于并行解耦的编码器-解码器架构（Parallel-Decoupled Encoder Decoder, PDED），改进了其注意力的查询输入矩阵（query），从而弥补了查询矩阵和键值矩阵（key）之间的图像信息，提高了模型的鲁棒性。
尽管注意力机制的特征表示能力比卷积和循环模块强，但注意力机制可以让模型在全局特征的角度上进行计算，适合长度适中的特征序列，但其计算复杂度随着序列长度的增加而成平方及增长，不仅如此，其模型参数规模也远大于卷积和循环模块。

*大语言模型辅助网络*。在大语言模型（Large-Language Model, LLM）兴起后，很多与文本相关的任务都可以借助大语言模型实现。由于大语言模型使用大量的数据集进行训练，而模型也拥有巨大的参数量，因此训练出来的大语言模型有着很强的特征表示能力。早期的大语言模型如BERT~@bert 基于Transformer~@transformer 编码器设计了双向编码的无监督训练，其参数量达到了1.1亿（110 million~@bert ）。后续的GPT系列~@gpt 模型则使用自监督的方式进行训练，其中GPT-3~@gpt3 的参数量达到了1.75千亿（175 billion~@gpt3）。在研究图像和文本之间的关系中，CLIP~@clip 多模态模型使用对比学习，同时得到了文本和图像的特征表示编码器。大语言模型中拥有的大量参数使其具有很强的特征的表示能力，因此目前有研究人员直接使用大语言模型的编码器对下游任务中的数据集进行编码表示，并设计模型直接学习编码后的特征。
使用预训练的大模型可以在文本识别任务中取得较好性能。例如在CLIP4STR~@clip4str 工作中借助了视觉语言模型（Vision-Language Model, VLM），认为当前大部分识别器是基于单模态进行训练的，而VLM给予了文本和对应图像的信息，该模型通过视觉解码器和混合模态解码器的预测结果优化，在11个基准测试集中达到了较高的识别准确率。
与此同时，由于大语言模型含有丰富的上下文语义信息，因此有研究人员使用大模型进行文本预测结果的优化。例如ABINet~@abinet 设计了两个分支处理场景文本识别，其一是以残差网络和Transformer为基础架构，并且带有位置注意力的视觉分支，其二则是以CLIP为基础的，用于优化预测文本的语言模型分支。实验发现使用语言模型进行结果的迭代优化可以在基准数据集上达到较高识别准确率。
正如前文所述，使用预训练的大语言模型作为特征编码器或预测优化器，可以达到较高的识别准确率，同时由于大语言模型的强语义性，识别的结果绝大多数依然带有语义性，即识别结果是正确的单词。但在场景文本中，存在许多无上下文语义的文本，因此在使用大语言模型时需要权衡语义信息和图像信息之间的关系。

== 场景文本图像超分辨率

// * NOTES: interpolation based, attention based, diffusion based
// 0. introduce pixel shuffle module
// 1.interpolation based: bilinear and bicubic
// 2.attention based: TATT, DPMN,
// 3.diffusion based: SR3

#figure(
  image("fig/srer_arch.svg", width: 75%),
  placement: auto,
  caption: [STISR超分器基本架构。其中虚线表示可选项]
)<fig2-sr>

场景文本图像超分辨率（STISR）的目的是提高低分辨率文本图像的质量，STISR能大大提升前文提到的STR任务对低分辨率文本的识别率，因此对该领域的研究对自动驾驶等下游任务的发展有着重要意义。与单张图像的超分辨率略有不同，STISR需要弥补图像特征和文本语义信息之间的模态差异，STISR的工作包含了文本的语义信息，在增强图像分辨率的同时不能破坏文本结构，因此STISR的约束条件比普通单张图像超分辨率更强。如@fig2-sr 所示，目前常用的模型框架通常包含一系列的编码器和特征学习模块，末尾通常使用像素重排模块~@pixelshuffle 提高图像分辨率。目前，已经有很多研究人员在该领域做出了巨大贡献，例如有研究人员将文本先验知识引入图像的特征学习模块中，用先验特征引导超分辨率，还有研究人员在SR图像的基础上进一步做结果的优化。本节先介绍传统插值的图像超分辨率方法，接着以该方法为基础，引出当前该领域的相关工作。

*插值*。插值主要目的是根据给定数据点，估计给定点附近的值。用于图像超分辨率的插值方式包括最近邻插值、双线性插值和双三次插值等方式。
如@fig2-2-1 假设原始图像大小为 $(h, w)$，以将图片分辨率放大到原来的*两倍*为例，即超分辨后的图像大小为 $(2h,2w)$，分别讨论三种超分辨率方式的效果。对于一个像素点，最近邻插值忽略其周围像素值，直接在周围扩展该像素的值，其效果如@fig2-2-1 （c）所示，可以发现其锯齿化比较严重。而双线性插值会考虑到以该像素点为顶点的周围像素值，假设四个顶点分别为 $A(x_1,y_1),B(x_1,y_2),C(x_2,y_2),D(x_2,y_1)$，这些点所代表的像素值分别由 $f(A),f(B),f(C),f(D)$ 给出，则该四个顶点间任意一个点的像素值 $P(x,y)$ 由公式@eq2-2-1 @eq2-2-2 和@eq2-2-3 给出。其插值效果如@fig2-2-1 （d）所示。相较于最近邻插值，双线性插值在一定程度上减轻了锯齿化。而双三次插值插值一次使用的像素点个数是双线性插值的 $4$ 倍，即使用 $16$ 个像素值估计一个像素值，类似于双线性插值其基本思想是使用三次多项式函数拟合给定的四个像素值，并通过一阶导数和二阶混合偏导保证连续性，从而得到 $16$ 个方程组成的方程组，解出 $16$ 个顶点对应的权重。双三次插值的效果如@fig2-2-1 （e）所示，可以看出其效果比双线性插值多了更多的图像细节。

#figure(
  image("fig/fig2-2-1.svg", width: 75%),
  placement: auto,
  caption: [（a）表示原始高分辨率图像 $(2h,2w)$，（b）表示压缩后大小为 $(h,w)$ 的图像等比放大后的低分辨率图像 $(2h,2w)$，（c）表示使用最近邻插值方式的超分图像 $(2h,2w)$，（d）表示使用双二次插值的超分图像 $(2h,2w)$，（e）表示使用双三次插值的超分图像 $(2h,2w)$]
)<fig2-2-1>

$
f(P_y_1)=(x_2-x)/(x_2-x_1) f(A) + (x-x_1)/(x_2-x_1) f(B)
$ <eq2-2-1>

$
f(P_y_2)=(x_2-x)/(x_2-x_1) f(D) + (x-x_1)/(x_2-x_1) f(C)
$ <eq2-2-2>

$
f(P)=(x_2-x)/(x_2-x_1) f(P_y_1) + (x-x_1)/(x_2-x_1) f(P_y_2)
$<eq2-2-3>

#h(2em) 使用插值的方式进行图像超分辨率，是在图像的宽和高两个维度上进行像素扩展，在使用深度学习模型进行超分辨率时，需要先将LR图像插值到SR图像，再使用卷积、注意力机制等模块增强其分辨率。而近年来提出的像素重排（PixelShuffle）模块~@pixelshuffle 可以直接在LR图像上提取特征，它的主要思想是将通道维度特征重组到宽和高维度上，该方式既能提高图像分辨率，又能在通道维度上提取特征，是当前网络增强图像分辨率的主要方式。如@fig2-2-2 所示，像素重排主要基于网络提取的特征图，特征图的通道维度上是具有超分辨像素信息的特征，通过重排后，通道维度的像素信息补全到宽和高两个维度，从而增强图像分辨率。与传统的插值方式相比，这种在通道维度上提取特征的方式可以减少网络传播中特征图的大小，同时也更适合具有卷积模块的网络在通道层面上提取特征的方式。基于该模块，有很多研究人员提出了新的超分辨率模型，并在基准数据集上取得了较好的结果。

#figure(
  image("fig/fig2-2-2.svg", width: 75%),
  placement: auto,
  caption: [像素重排（PixelShuffle）模块的主要操作原理。原特征图的通道维度被重排至宽和高两个维度，从而提高图像的分辨率]
)<fig2-2-2>

*模型*。使用卷积和循环网络搭建模型的代表性工作是TSRN~@tsrn，同时该模型的作者也是STISR任务的提出者，并创建了新的数据集用于训练和测试。该模型的主要思路是残差学习，模型使用了重复的超分子模块SRB~@tsrn 对超分图像的残差进行学习后，将残差与浅层特征进行加和，最后使用像素重排输出较高分辨率的图像。在整个网络的特征传播中，特征图的大小保持一致且与LR图像大小相同，模型仅在通道维度上进行特征学习，与前文介绍的像素重排相互印证。
TSRN是STISR领域的开山之作，因而有研究人员延续其特点，提出了新的架构。例如TATT模型~@tatt 在此基础上增加了注意力机制，目的是混合先验文本信息和图像特征，在一定程度上用文本的先验特征引导超分辨率的进行。TATT模型在LR图像输入时，通过预训练识别器对LR图像先进行一次识别得到预测的文本标签，同时用多个卷积模块提取LR图像的图像特征，再将二者输入到带有交叉注意力的特征融合模块，最后通过TSRN的主干网络输出超分辨率图像。与此思想类似的工作还包括TPGSR~@tpgsr ，该模型同样使用了文本先验模块对超分辨率模块进行引导，并同时使用由高分辨率图像产生的文本先验约束LR图像的文本先验，以便生成更可靠的文本先验。由于大部分超分网络具有一定的缺陷，有研究人员基于这些超分网络，设计了即插即用的超分辨率模块用于提高预训练超分模型的性能。DPMN~@dpmn 则从优化的角度出发，在原有的超分辨率模型的基础上添加即插即用的性能提升模块，将现有的超分辨率模型输出的SR图像作为DPMN模块的输入，输入PGRM（Prior-Guided Refinement Module）后使用预训练的ViT提取先验信息，并对其做像素重排，将多个PGRM的输出再进行组合，最终得到分辨率更高的SR图像。PCAN~@pcan 则训练出用于提取超分语义信息的注意力模块，并提出了新的损失函数用于模型的监督训练。STT~@stt 模型则使用注意力机制关注单个文本的字形的细节特征，并在特征图层面上用损失函数进行监督。而C3-STISR~@c3 则从不同特征理解的角度出发，利用注意力机制权衡识别、图像和语义三种理解，在基准数据集上超过同期模型。而随着DDPM~@ddpm 等扩散模型（Diffusion Model）的提出，研究人员发现该模型在图像恢复上面有一定的效果，因此部分研究人员将DDPM及其衍生模型引入STISR领域，扩充了文本图像的超分辨率模型类型。DDPM在原图上进行多步加噪和多步去噪过程，用模型预测加的噪声，最终推理时用纯噪声逐步减去预测的噪声，实现图像的生成。由于原始的DDPM生成效果带有随机性，因此SR3~@sr3 基于隐式扩散模型（Stable Diffusion）将LR图像作为条件概率中的条件，引导生成图像的效果，最终在单张图像超分上取得了很好的效果。在STISR任务上，TextDiff~@textdiff 首次使用扩散模型进行残差学习，在基准数据集上取得良好效果。

== 数据集

#figure(
  placement: auto,
  grid(
    columns: 3,
    gutter: 1%,
    [#subfigure(
      image("fig/stat_ds.svg"),
      caption: [各个数据集样本量]
    )<fig2-2-3a>],
    [#subfigure(
      image("fig/stat_char.svg"),
      caption: [标签文本类型]
    )<fig2-2-3b>],
    [#subfigure(
      image("fig/stat_length.svg"),
      caption: [标签文本长度]
    )<fig2-2-3c>],
  ),
  caption: [TextZoom数据集标签特征统计]
)<fig2-2-3>

#figure(
  placement: auto,
  grid(
    columns: 2,
    gutter: 15%,
    [#subfigure(
      image("fig/train_stat.svg"),
      caption: [训练集PSNR与SSIM分布图]
    )<fig2-2-4a>],
    [#subfigure(
      image("fig/test_stat.svg"),
      caption: [测试集PSNR与SSIM分布图]
    )<fig2-2-4b>],
  ),
  caption: [TextZoom数据集图像特征统计。PSNR和SSIM越高表示LR图像质量越高]
)<fig2-2-4>

对于单独的STR任务而言，目前绝大部分识别器需要在大规模的人造数据集和真实数据集上进行训练，得到的模型通常包含了与文本相关的语义信息，可以在诸多下游任务的数据集上进行微调。用于大规模训练的人造数据集有包含了900K样本量的MJSynth~@mjsynth 和包含了800K样本量的SynthText~@synthtext。当前已经有很多STR工作在这些数据集上进行训练，用于后续模型的微调。而用于STISR和STR双任务的数据集目前常用的是TextZoom~@tsrn ，该数据集由两个单张图片超分辨率的基准数据集RealSR~@realsr 和SR-RAW~@sr-raw 筛选和截取而成，其中标签的分布如@fig2-2-3 所示，图像的分布如@fig2-2-4 所示。TextZoom分为训练集（包含验证集）、简单难度的测试集、中等难度的测试集和困难难度的测试集，各个数据集的样本量如@fig2-2-3a 所示。如@fig2-2-3b 所示，在文本字符角度，数据集的标签文本类型包含了数字和大小写英文字母，每个图像的标签不仅包含数字，也有可能同时包含大小写的英文字母。如@fig2-2-3c 所示，数据集大部分的标签长度在$10$以内，且各个长度的分布比较均匀。
在图像方面，图像的质量由PSNR（Peak Signal Noise Ratio）和SSIM（Structural Similarity Index Measure）反映，指标的值越高，则两张图像越接近。在@fig2-2-4 中，本文统计了LR图像相对于HR图像的PSNR和SSIM，可以发现训练集和测试集在分布上有一定的相似性，因此使用TextZoom训练集作为模型的训练数据。而由于测试集图像质量比较分散，因此使用不同难度的LR图像作为模型性能的评估数据集也是合理的。

同前文所述，使用在大规模人造数据集和真实数据集上预训练的识别器进行微调可以给模型带来一定的先验语义特征，TextZoom数据集和MJSynth数据集在标签和图像分布上存在一定差异，适合用作微调数据集。因此本文识别器采用一种在MJSynth数据集上预训练后的识别器作为主干，并研究了相应的微调方法，最终取得较好的识别性能。

= 模型原理与设计

本章首先介绍模型的总体架构及其形式化描述，再分别详细介绍模型各个模块的设计和实现效果，最后给出模型训练所使用的损失函数以及训练和推理算法。

== 模型总体架构
//TODO:
#figure(
  image("fig/arch.svg", width: 100%),
  // placement: auto,
  caption: [模型整体架构。模型编码器和解码器（灰色虚线框内部）在各个阶段都处于激活状态，特征增强部分（红色虚线框内部）只在特征增强阶段被激活和训练。在编码器和后处理模块中使用了额外的长残差连接]
)<fig3-arch>

不失一般性，模型的输入和输出都是以一个批量大小进行，超分辨率的倍数为$2$，假设输入的LR低分辨率图像用 $I_l$ 表示，对应的HR高分辨率图像用 $I_h$ 表示，而超分辨率模型的输出SR为 $I_s$，图像对应的真实标签和识别器预测标签在嵌入模型后，分别为 $S_t, S_p$。图像和嵌入标签维度分别满足：
$I_l in bb(R)^(B times C times H times W), I_h in bb(R)^(B times C times 2H times 2W), I_s in bb(R)^(B times C times 2H times 2W), S_t in bb(R)^(B times L_t), S_p in bb(R)^(B times L_p)$
其中 $B$ 表示输入和输出数据的批量大小，$C$ 表示输入和输出图像的通道数，通常为 $3$，而 $H,W$ 分别表示LR图像的高和宽，由于放大倍数是 $2$，因此SR图像和HR图像的高和宽分别是 $2H,2W$，对于标签，$L_t,L_p$ 分别表示了真实标签的长度和预测标签的长度。

该模型的目标是通过训练集 $cal(S)={(L_t,I_h,I_l)_i}_(i=0)^N$ 训练模型 $cal(F)_theta$，使其能够以 $I_l$ 为输入，预测对应的标签 $L_p$ 和重建SR图像 $I_s$，即

$
cal(F)_theta (I_l)=(L_p,I_s)
$

从目前研究中发现，现有的模型多是基于串行结构搭建，即先对LR图像进行超分辨率，提高文本图像的分辨率，再用识别器进行文本预测。类似这一类的串行结构理解简单、训练更为方便，同时有很多研究人员达到了部分预期效果。然而这种框架经常会带来一些问题：
+ 首先，由于识别结果的准确率大多基于SR图像的质量，因此如果超分辨率模块的输出结果并不可靠，从而生成错误的超分辨率图像，就极有可能导致后续识别器预测出错误的文本。
+ 其次，两个任务是串行关系，因此如果不使用梯度截止等方法，后一个模块的损失在进行传播时会影响到前一个模块参数的更新，从而导致超分辨率模块被识别损失监督，原有的超分损失受到影响，从而生成的SR图像并不完全有着较高的保真度。

#h(2em) 为解决以上问题，本文使用了一种新型的多任务并行架构，如@fig3-arch 所示，该模型的识别模块和超分模块处于并行关系，二者共享图像的编码特征，但解码特征相互独立。在使用多个损失函数进行监督训练时，两个解码器会更加注重于更新参数使得与其对应的损失降低，而不会受到另一个损失的影响。其次由于识别器从特征层抽取特征，受到超分辨率效果的影响较小，两个任务的耦合度大大降低。为了进一步提升图像特征的质量，该模型还设计了特征增强模块，提高LR图像的特征质量。设编码器为 $cal(E)_theta$，识别器为 $cal(D)^r_theta$， 超分器为 $cal(D)^s_theta$，特征增强模块为 $cal(A)_theta$，编码器输出特征为 $F_e in bb(R)^(B times C_e times H_e times W_e)$，特征增强模块输出为 $F_a in cal(R)^(B times C_a times H_a times W_a)$ （其中 $C_x,H_x,W_x$ 分别表示中间特征的通道数、高度和宽度）该架构可以由@eq3-1 表示：在推理阶段，模型的输入为LR低分辨率图像，先经过编码器进行初步的特征提取，再由特征增强模块提取LR特征潜在的超分辨率图像特征和文本特征，最后将含有超分辨率和文本识别混合特征的张量图输入不同任务的解码器，从而输出识别文本和SR超分辨率图像，实现模型对LR图像的识别和超分辨率。

$
cal(F)_theta (I_l) &= (cal(D)^r_theta (F_a),cal(D)^s_theta (F_a)),\
F_a &= cal(A)_theta (F_e),\
F_e &= cal(E)_theta (I_l)
$<eq3-1>

#h(2em) 整个模型的训练方式可以分为两个阶段：预训练阶段和特征增强阶段。

*预训练阶段*。该阶段特征增强模块不接入模型。模型主要使用降采样后的HR图像训练识别器分支和超分辨率分支。该阶段目的是将HR图像的编码特征保留在编码器中，让编码器学习到HR图像的特征解码方式，以便后续能以类似的方式提取LR图像的特征。其次是预训练超分器和识别器，提高超分器的重建效果和识别器对编码特征的识别性能。该阶段训练主要为后续使用LR图像训练提供模型的先验初始化参数。

*特征增强阶段*。该阶段主要训练特征增强模块，需要将其接入模型。特征增强思路是使用LR图像和HR图像共同进行训练，使得该模块能输出与HR编码后特征相近的超分辨率特征。同时，该阶段需要微调解码器的两个分支，以便弥补SR特征和降采样HR特征之间的分布差异。


== 编码器模块

编码器主要负责对齐图像和混合特征。

#figure(
  image("fig/encoder.svg", width: 75%),
  caption: [编码器模块，虚线部分为SRB]
)<fig3-encoder>

*图像中心对齐*。由于数据集通过失焦采样真实的场景文本，LR图像中心和HR图像中心会出现一定的偏移，因此模型在编码器部分设计了空间变换网络（Spatial Transformer Network, STN）~@stn 用于对图像做空间尺度的变换，将LR图像与HR图像中心对齐。同时编码器还使用薄板样条插值（Thin Plate Spline, TPS）~@tps 扩大STN变换的灵活性。STN模块实现对图像变换的预测，增强卷积对图像剪切、旋转和仿射等变换的鲁棒性，TPS模块使用插值的方式，通过关注图像控制点的变换，实现估计图像除控制点以外其余点的位置。该部分的基本流程为：先通过STN预测输入图像变换后控制点的位置，再将控制点和图像输入TPS实现图像的变换。

*混合特征*。编码器的定位特殊，作为不同任务的前置特征提取模块之一，编码器的设计和训练需要多次实验测试。由第二章@fig2-rec 可知，通常的识别器包含图像特征编码器和后续的序列特征学习模块，而由第二章@fig2-sr 可知，通常的超分器同样包含图像特征编码器和后续的超分图像特征学习模块。因此本文设想通过构造一种以卷积为主的特征提取编码器，同时实现文本特征的提取和超分特征的提取。预训练阶段，编码器输入的是HR高分辨率图像，超分模块的重建任务较为简单，该阶段需要注重文本特征的提取；特征增强阶段，需要在编码器后接入特征增强模块，此时模型的输入是LR低分辨率图像，此时预训练完成的编码器会先对LR低分辨率图像进行特征提取，接着将LR低分辨率特征进行增强，输出适应预训练解码器的超分特征，该超分特征含有文本信息。因而在编码器设计部分，本文主要利用编码器实现图像文本信息的浅层提取，而超分变率特征主要由后续的特征增强模块实现。

该模块的设计如@fig3-encoder 所示，输入的图像先统一调整大小，使其与推理时输入的LR图像大小一致，具体而言，如果输入的是大小为 $(2h,2w)$ 的HR图像，则使用最近邻插值调整大小到 $(h,w)$；而如果输入是LR图像，则不进行大小调整。调整大小后的图像输入中心对齐模块，经过STN和TPS的处理后再进入卷积网络映射到特征层，接着使用SRB~@tsrn 整合图像特征和文本特征。其中SRB的设计如@fig3-encoder 中间虚线部分所示，进入BiGRU模块前的特征图进行维度调整：先切割为纵向排列的条状特征（设该操作为$f$），再将这些条状特征依次连接（设该操作为$g$），即

$
g(f(F)):bb(R)^(B times C times H times W) -> bb(R)^((B H) times W times C)\
f(F):bb(R)^(B times C times H times W) -> bb(R)^(B times H times W times C)\
g(F):bb(R)^(B times H times W times C) -> bb(R)^((B H) times W times C)
$

#h(2em) 维度调整能够实现图像特征和文本特征的融合，相邻的条状特征更有可能对应于一个文本。处理后的特征图送入BiGRU进行特征提取，输出的特征映射回原维度。最后将输出特征与卷积特征通过残差进行连接，输入到下一个模块。为了提高编码器特征提取能力，编码器内部堆叠了两个SRB模块。预训练阶段，编码器输出的特征包括两部分，第一部分的特征会经过像素重排输入识别器分支，第二部分不经过像素重排，送入超分辨率分支继续处理超分辨率特征。特征增强阶段和推理阶段，编码器输出的特征是LR特征，不会经过像素重排，直接输入特征增强模块做进一步的特征提升。编码器的训练和解码器的训练方式一致，使用多个损失函数和不确定性加权，这一点将在本章最后一节进行讨论。

== 特征增强模块

特征增强模块主要负责增强的编码器输出的LR特征。

#figure(
  image("fig/augment.svg", width: 75%),
  caption: [特征增强模块]
)<fig3-augment>

与编码器类似，特征增强模块的主干网络使用SRB进行堆叠，但在整个模块中使用了更长的残差连接，并且该模块训练方式与编码器有所不同。如@fig3-augment 所示，特征图输入后先保留一份备份用于残差连接，再经过堆叠的$N$个SRB模块提取特征，接着通过卷积层融合图像和文本特征，最后使用残差连接将输出的特征图与进入网络的特征图相加，同3.2节所述，模块分别输出大小不同的特征图分别输入识别器和超分器。为了使用合适个数的SRB模块，本文在第四章对增强模块的SRB个数做了消融实验。

== 双任务解码器

双任务解码器分为识别器和超分器。识别器负责将输入的特征图解码为文本。超分器负责将特征图重建为SR超分辨率图像。

#figure(
  image("fig/recognizer.svg", width: 75%),
  caption: [识别器模块]
)<fig3-recognizer>

*识别器*。常见的工作中可以使用任何预训练识别器作为主干，只需要微调其分类头即可在相应数据集上实现较高的准确率。预训练模型中的编码器部分是用于提取图像特征的关键部分，该部分能提取多量、浅层的细节特征，而解码器部分用于处理这些浅层特征，并逐步整合为完整的深层特征，最后分类头和激活函数将深层特征映射为几率（logits）。本文的识别器受到CRNN~@crnn 的启发，在CNN和LSTM之间加入了MLP和残差连接，同时增强了模型分类头，如@fig3-recognizer 所示。在消融实验中，本文将讨论微调预训练识别器和重新训练识别器之间的差异。

#figure(
  image("fig/srer.svg", width: 75%),
  caption: [超分器模块]
)<fig3-srer>

*超分器*。如@fig3-srer 所示，超分器先将输入的特征通过卷积层进行升维

$
cal(D)_("conv"): bb(R)^(B times C times H times W) -> bb(R)^(B times (C s^2) times H times W)
$

其中 $s$ 代表超分辨率的倍数。经过一层Mish激活函数~@mish 后进入像素重排模块，按照 $s$ 倍数放大特征图的高和宽

$
cal(D)_("pixel_shuffle"): bb(R)^(B times (C s^2) times H times W) -> bb(R)^(B times C times H s times W s)
$

最终再由卷积层映射到原始图像的通道数。

== 损失函数

模型使用多个损失函数进行监督训练，并学习不确定性权重（Uncertainty Weight Loss）~@uncertainty-loss 来权衡不同任务之间的比重。

*识别损失$cal(L)_r$*。识别器使用CTC损失函数~@ctc 进行监督训练，该损失常用于文本序列预测的监督任务。在序列模型中，采用交叉熵损失或均方误差损失时，经常需要将输出的预测值与真实值标签对齐，这种对齐方式工作量大；除此之外，预测结果中可能存在空白字符和连续字符，空白字符可能由图像中两个文本间的空白区域预测得到，而连续字符则可能是图像中同一字符被多次识别，也有可能是文本中原本就存在连续字符；因此解码器的工作量较大。CTC损失通过在标签的相邻字符中添加空白字符，在解码时先合并相同字符，再删去空白符得到预测标签，由于两个连续字符之间必然会存在空白字符，因而不会造成预测歧义。根据隐马尔可夫的前向后向算法可以得到该损失函数反向传播时的更新梯度为

$
g_t = (partial P_theta_t (hat(Y) | X)) / (partial theta_t) = sum_(i,j,c) (partial p_(i,j)(c)) / (partial theta_t) alpha_(i,j) beta_(i,j+1)
$

该梯度的具体推导见附录。

*超分损失$cal(L)_s$*。超分器使用两类损失共同监督训练。第一部分为MSE均方误差损失

$
cal(L)_2 = 1/n sum_(i=0)^n (I_h (y_i) - I_s (y_i))^2
$

该损失在图像像素空间对超分图像 $I_s (y) = cal(F)_theta (y)$ 的输出进行了约束，监督图像在颜色上进行恢复。

第二部分基于梯度剖面先验（Gradient Profile Prior Loss, GPP）~@gpp 在图像的梯度场上设计了损失函数

$
cal(L)_("GP") = 1/n sum_(i=0)^n ||nabla I_h (y_i) - nabla I_s (y_i)||_1
$

其中 $nabla I$ 为图像的梯度，即 $nabla I = ((partial_x) / (partial x) I, (partial_y) / (partial y) I) = sqrt(((partial_x) / (partial x) I)^2 + ((partial_y) / (partial y) I)^2) arrow(n)$（$arrow(n)$ 是同方向单位向量）。

图像梯度反映了图像的锐化程度，梯度场中的梯度剖面越小，说明该图像的边界越明显~@gpp 尖锐，在一定程度上说明该图像越清晰。最终的超分损失为二者的加权和

$
cal(L)_s = lambda_1 cal(L)_2 + lambda_2 cal(L)_("GP")
$<eq3-sr>

其中 $lambda_1,lambda_2 in bb(R)$，在后续的消融实验中会进行该值的讨论。

// *特征损失$cal(L)_f$*。// TODO: ing waiting for result, maybe no feature loss

*多任务损失权重*。本文的模型涉及到多个损失函数，因此需要一种权衡任务规模、梯度大小的加权方式，实现模型使用多任务损失进行端到端训练。一种常用的加权方式是各个梯度间的线性组合

$
cal(L) = alpha_1 cal(L)_r + alpha_2 cal(L)_s
$<eq3-avgloss>

这种加权方式简单，并且能够自行根据任务规模设置权重系数，但缺点是不能根据训练任务进行的状态实时更新权重，忽略了训练时各个梯度更新快慢不一致的特点。

而使用不确定性加权~@uncertainty-loss 的方式则可以动态学习损失间的加权系数，加权系数中含有带学习的参数 $sigma$，可以随着模型参数的更新同不更新，从而调整不同任务的损失函数权重，使用不确定性加权后的总损失可以写为

$
cal(L) = 1/(2 sigma_r^2) cal(L)_r + 1/(2 sigma_s^2) cal(L)_s + alpha log (1+sigma_r^2) + alpha log (1 + sigma_s^2)
$<eq3-weightedloss>

其中 $sigma_1,sigma_2$ 是待学习的参数，$alpha$是惩罚项，防止$sigma_1,sigma_2$过大，在训练前进行指定。该损失的具体推导可以参考附录。

= 实验结果与分析

本章主要对本文搭建的模型进行性能分析。本章首先从模型总体推理性能出发，将模型与部分现有模型（State of the art, SOTA）进行对比，着重分析每个模型推理性能的差异。其次本章设计了多个消融实验（Ablation Study），对模型中使用到的模块和训练方法进行了研究。

== 评估指标

本文使用了多个评估指标进行实验性能对比。

*峰值信噪比（PSNR）*。PSNR主要用于评估超分辨率任务的性能，该指标一般用于衡量图像经有损压缩后的重建效果。其计算方式如@eq4-psnr 所示。

$
"PSNR"(I_s, I_h) = 20 dot lg ("MAX"(I_h) / sqrt("MSE"(I_h - I_s)))\
"MSE"(I_h - I_s) = 1 / (r c) sum_(i=0)^r sum_(j=0)^c ||I_h (i,j) - I_s (i,j)||^2
$<eq4-psnr>

其中 $"MAX"(I_h)$ 表示原图像 $I_h$ 的最大像素值，$"MSE"(I_h - I_s)$ 表示原高分辨率图像 $I_h$ 和压缩后重建的超分辨率图像 $I_s$ 之间的均方误差。不同于普通信噪比（SNR）的计算方式，PSNR的分子使用图像信号的最大值而不是整个图像的模方，表示图像的重建效果更加关注图像的高强度区域的抗噪能力。

*结构一致性指标（SSIM）*~@ssim。SSIM主要用于衡量超分辨率任务的性能，该指标通过比较超分辨率图像$I_s$和原始高分辨率图像$I_h$间的亮度$l(I_s,I_h)$、对比度$c(I_s,I_h)$和结构$s(I_s,I_h)$，对图像间的一致性进行衡量。其主要用像素值的统计量衡量图像的视觉特征，即图像像素值的均值可以反映图像的明暗程度，像素间的方差可以反映图像的对比度，而两张图像之间的协方差则可以反映像素值之间的关联程度。三个子指标的计算方式如@eq4-ssim1 所示。

$
l(I_s,I_h) = (2 mu_s mu_h + c_1) / (mu_s^2 + mu_h^2 + c_1)\
c(I_s,I_h) = (2 sigma_s sigma_h + c_2) / (sigma_s^2 + sigma_h^2 + c_2)\
s(I_s,I_h) = (sigma_(s h) + c_2/2) / (sigma_s sigma_h + c_2 / 2)
$<eq4-ssim1>

其中 $mu_s, mu_h$ 分别表示超分辨率图像 $I_s$ 和高分辨率图像 $I_h$ 像素值的均值，$sigma_x,sigma_y$ 分别表示 $I_s$ 和 $I_h$ 像素值的标准差，而 $sigma_(s h)$ 表示 $I_s$ 和 $I_h$ 之间的协方差，为了防止分母出现 $0$ 项，$"SSIM"$ 的计算中添加了 $c_1,c_2$ 两个变量。

因此总的$"SSIM"$指标可以由@eq4-ssim2 计算得到。

$
"SSIM"(I_s, I_h) = l(I_s,I_h) dot c(I_s,I_h) dot s(I_s,I_h)
$<eq4-ssim2>

#h(2em) *准确率（ACC）*。准确率主要用于衡量识别任务的性能，计算方式比较简单，如@eq4-acc 所示，即识别正确的标签数占总标签数的比例。

$
"ACC" = N_"correct" / N_"total"
$<eq4-acc>

值得注意的是，衡量准确率时，模型以标签文本整体而非单个字符作为识别统计的基本单位。

*浮点运算数（FLOPs）*。用于衡量一个模型进行一次实例计算时需要进行的浮点运算次数。该指标可以反映一个模型的计算复杂度，FLOPs越大，表示模型进行一次推理所需要的浮点运算数越多，模型的计算复杂度也就越大。

*参数量（Params）*。模型参数量用于反映一个模型的大小。

*每秒推理图片张数（FPS）*。FPS能够衡量模型推理速度，FPS越大表示推理速度越快。

== 实现细节

//TODO: 添加StepLR引用，如果有的话，补充学习率的图，灰度掩码的图，修改epoch
本文所实现的模型使用单张显存大小为$24$G的 NVIDIA RTX 3090 显卡完成训练和推理。批量大小设置为512组数据，其中一组数据包含$0~9$和$a~z$以及空白字符组成的文本标签、$32 times 128$ 像素大小的高分辨率图像和 $16 times 64$ 像素大小的低分辨率图像。本文在$3$通道图像的基础上添加了一层灰度掩码，不同分辨率图像的灰度掩码如图\@所示。整个模型的输入是$4 times 16 times 64$ 大小的低分辨率图像。整个训练过程一共遍历$300$次训练集。模型使用AdamW~@adamw 优化器进行参数优化，并设置$beta in [0.5,0.999]$~@tsrn。由于模型使用了多个包含BiLSTM和BiGRU的模块，为了防止梯度过大，在训练时加入了梯度裁剪，超分器分支的梯度裁剪为$0.25$，识别器分支梯度裁剪为$5$。学习率更新策略采用StepLR，即每隔固定的步长，将学习率按一定比例进行衰减。对于模型参数而言，初始学习率设置为 $5 times 10^(-4)$，学习率衰减比例设置为 $0.5$，每遍历$30$次训练集进行一次衰减；而对于可训练的损失权重而言，与模型参数一致，初始学习率和衰减比例分别设置为 $1 times 10^(-5)$和$0.8$，但衰减步长为模型参数的 $1.5$ 倍，以便在模型训练初期以较大学习率调整任务权重，同时避免两组学习率以较大幅度衰减时模型出现的不稳定现象，学习率衰减曲线如@fig3-lr 所示。为了保证训练开始时两种损失策略对不同任务有相同的初始权重，在@eq3-avgloss 中，设置 $alpha_1=alpha_2=1.0$，在@eq3-weightedloss 中，设置 $sigma_r=sigma_s=sqrt(2)/2 approx 0.7, alpha = 0.1$。超分辨率任务损失函数@eq3-sr 中的 $lambda_1=20,lambda_2=1 times 10^(-4)$~@tsrn，本章后续将对该值进行消融实验。

#figure(
  image("fig/lr.svg", width: 50%),
  caption: [StepLR学习率衰减策略。横轴为训练过程，纵轴(对数刻度)为学习率大小]
)<fig3-lr>

由于是多任务训练，因此本文的PSRec模型在训练期间分别保存了准确率最优和PSNR最优的模型参数，在进行模型层面的对比时，如无特殊说明，本文的PSRec模型使用的是PSNR最优的模型参数。两种模型参数的指标对比差异可以查阅附录。除此之外，PSRec的识别器采用第三章提到的添加旁路BiLSTM模块和MLP的方式进行微调，增强模块中包含 $3$ 个SRB，训练的超分辨率损失函数权重为 $lambda_1=20,lambda_2=1 times 10^(-4)$，整个模型使用不确定性加权的方式进行损失之间的比例调控，权重参数初始值为 $sigma_r=sigma_s=sqrt(2)/2 approx 0.7$，权重的惩罚值为 $alpha=0.1$，此时两个任务的权重为 $1$。

== 模型性能

本节主要将本模型PSRec与TSRN~@tsrn 的超分性能和CRNN~@crnn 的识别性能以及它们串行连接组成的模型进行对比。在本实验中主要使用第二章提到的TextZoom~@tsrn 数据集衡量模型性能，并通过本章第一节提到的指标多方面评估模型效果。本节所使用的TextZoom测试集分为三种难度，如~@tab4-1 所示，Easy（E）表示简单难度，Medium（M）表示中等难度，Hard（H）表示较困难难度，Avg（A）表示在三种难度测试集上的加权平均，权值由三种难度数据集的数量给出，分别为$[0.3699, 0.3227, 0.3074]$。

在超分辨率方面，本文的PSRec模型的超分辨率任务分别使用@eq3-sr 提到的$L_s=lambda_1 L_2 + lambda_2 L_("GP")$超分损失和单独的均方误差损失$L_s=L_2$进行训练，并与相同损失训练的TSRN~@tsrn 模型进行TextZoom测试集中低分辨率图像重建效果的对比，衡量指标选择PSNR和SSIM。

//TODO: 补充，正在训练
#figure(
  placement: auto,
  tablex(
    auto-lines:false,
    columns:(2fr,2fr,1fr,1fr,1fr,1fr,1fr,1fr,1fr,1fr,),
    rows:2em,
    stroke:1pt,
    repeat-header:true,
    header-rows:2,
    align:center+horizon,
    hlinex(),
    [],[],colspanx(4)[PSNR],(),(),(),colspanx(4)[SSIM],(),(),(),
    hlinex(),
    [模型],[损失函数],[E],[M],[H],[*A*],[E],[M],[H],[*A*],
    hlinex(),
    [TSRN~@tsrn],  [$L_2$],         [22.33],[18.37],[19.46],[20.17],[0.8441],[0.6366],[0.7046],[0.7343],
    [TSRN~@tsrn],  [$L_2+L_("GP")$],[23.27],[18.67],[19.84],[20.73],[*0.8605*],[*0.6597*],[*0.7284*],[*0.7551*],
    [PSRec(Ours)], [$L_2$], [23.50], [19.86], [20.59], [21.43], [0.7817], [0.5241], [0.6172], [0.6480],
    [PSRec(Ours)], [$L_2+L_("GP")$], [*24.05*], [*20.01*], [*20.87*], [*21.77*], [0.8049], [0.5377], [0.6334], [0.6660],
    hlinex()
  ),
  kind:table,
  caption: [TextZoom数据集上三种难度测试集的PSNR/SSIM指标对比。其中$L_2$表示MSE均方误差损失，$L_("GP")$表示第三章提到的梯度剖面损失。二者的权重分别为$lambda_1=20,lambda_2=1 times 10^(-4)$。由于只比较超分辨率任务，该表格省略了PSRec模型损失函数中的识别损失$L_r=L_("CTC")$。]
)<tab4-1>

//TODO: 用具体的数字说明
由@tab4-1 可以看出，本文搭建的模型PSRec使用与TSRN相同的如第三章所示的超分损失 $L_s = lambda_1 L_2+ lambda_2 L_("GP")$ 进行训练后，PSRec在PSNR上的指标超过了TSRN，表明本文设计的模型对于低分辨率图像的重建效果高于TSRN。与TSRN模型不同的是，PSRec在训练时超分器时同时训练了识别器，而它们共用一个编码器特征，因此编码器可以提供部分的识别信息传递给超分器，对超分器性能有一定的提升。而相比于只使用 $L_s=L_2$ 损失进行训练的模型，在损失函数中添加梯度剖面损失可以增强模型的超分辨率性能。这是由于梯度剖面损失提供了图像的锐度信息，将其作为损失函数一部分可以在一定程度上强化图像的边缘特征，因此用含有该信息的损失函数训练的模型也能获得更高的PSNR。

//TODO: 用具体的数字说明
准确率方面，本文的PSRec模型使用CTC损失进行训练，并用多任务权重调整损失比例。其中识别器分别采用不同的微调策略：（1）dense：表示重新训练最后一层分类头，（2）mlp：表示将最后一层分类头更换为多层感知机并重新训练，（3）rnn：在原有BiLSTM基础上增加BiLSTM旁路，其结构与原有BiLSTM一致，输出采用原有结构和旁路结构输出的平均值，（4）rnn-mlp：结合（2）和（3）的方式。作为对比，本文同时在TextZoom训练集上微调了CRNN模型以及TSRN与CRNN的串行连接模型，除此之外本文还使用了不经过任何微调的CRNN模型在测试集上进行zero-shot推理，并将该推理结果作为各个模型的比较基准。@tab4-2 展现了本文并行连接的模型PSRec与CRNN以及串行连接的TSRN+CRNN在TextZoom测试集上比较的结果。

//TODO:
#figure(
  placement:auto,
  tablex(
    auto-lines:false,
    columns:(2fr,1fr,1fr,1fr,1fr),
    rows:2em,
    stroke:1pt,
    repeat-header:true, //换页时重复表头
    header-rows:1, //表头行数
    align:center+horizon,
    hlinex(),
    [模型和微调策略],[Easy],[Medium],[Hard],[*Avg*],
    hlinex(),
    [CRNN],               [0.2110],       [0.1284],       [0.1236],       [0.1575],
    [CRNN-dense],         [0.5860],       [0.4525],       [0.3150],       [0.4596],
    [CRNN-mlp],           [0.4158],       [0.2957],       [0.2137],       [0.3149],
    [CRNN-rnn],           [0.5990],       [0.4589],       [0.3142],       [0.4662],
    [CRNN-rnn-mlp],       [0.5842],       [0.4631],       [0.3105],       [0.4610],
    [TSRN+CRNN-dense],    [0.6504],       [0.5028],       [0.3589],       [0.5132],
    [TSRN+CRNN-mlp],      [0.6547],       [0.4638],       [0.3350],      [0.4949],
    [TSRN+CRNN-rnn],      [0.6429],       [0.4972],       [0.3574],       [0.5081],
    [TSRN+CRNN-rnn-mlp],  [0.6609],       [0.4837],       [0.3485],       [0.5077],
    [PSRec-dense(Ours)],  [0.7079], [0.5532], [0.3879], [0.5596],
    [PSRec-mlp(Ours)],    [0.6906], [0.5418], [*0.3939*], [0.5514],
    [PSRec-rnn(Ours)],    [0.7004], [*0.5631*], [0.3842], [0.5589],
    [PSRec-rnn-mlp(Ours)],[*0.7172*], [0.5447], [0.3887], [*0.5605*],
    hlinex()
  ),
  kind:table,
  caption: [TextZoom数据集上三种难度测试集的识别准确率。模型后的“-”表示使用的微调策略,若无，则表示不使用任何微调策略，直接在测试集上进行推理。]
)<tab4-2>

//TODO: 补充哪一种微调方式最好，肯定比不过TSRN+CRNN
由@tab4-2 可以看出，不进行微调的CRNN在各个测试集上进行推理的效果远远低于微调后的效果，表明本文所使用的微调方式可以带来性能提升。其中在原有模型的基础上增加BiLSTM旁路和多层感知机MLP的微调效果最好。对于串行方式连接的模型TSRN+CRNN，它先将输入的低分辨率图像进行超分辨率处理，得到超分辨率图像，再使用超分辨率图像对CRNN识别器进行微调。由@tab4-2 所示，这种方式在测试集的准确率高于使用CRNN进行识别的准确率，表明超分辨率模型的结果能对识别器性能带来提升。但通过串行方式连接超分器和识别器的识别准确率低于本文并行连接的模型PSRec，表明串行模型处理多任务时，识别器会受到超分器输出的影响，不同任务的耦合度较大，而本文搭建的模型PSRec实现的是多任务并行架构，识别器与超分器共用编码器特征，并分别进入不同的解码器进行处理，在解码过程中识别器不会受到超分器最后结果的影响，两个任务的耦合度较低，因而识别效果高于前两种方式。

#figure(
  placement: auto,
  tablex(
    auto-lines:false,
    columns:(2fr,1fr,1fr,1fr),
    rows:2em,
    stroke:1pt,
    repeat-header:true, //换页时重复表头
    header-rows:1, //表头行数
    align:center+horizon,
    hlinex(),
    [模型],[FPS(Hz)],[Params(M)],[FLOPs(G)],
    hlinex(),
    [TSRN+CRNN],  [109.91],       [11.9522],       [1.8782],
    [PSRec(Ours)],[*111.22*],       [*11.9516*],       [*1.8758*],
    hlinex()
  ),
  kind:table,
  caption: [串行连接的模型TSRN+CRNN和本文并行连接的模型在单张图像推理时间FPS、参数量和浮点运算数上的比较。其中微调策略都为rnn-mlp。]
)<tab4-3>

对于串行连接的多任务模型和并行连接的多任务模型，在模型规模、推理速度上也存在一定的差异。如@tab4-3 所示，在FPS、参数量和浮点运算数方面，同等微调策略下，并行连接的模型PSRec由于不同任务可以共用编码器特征，因此本文模型的参数量少于同等配置的TSRN+CRNN串行模型；与此同时，仅使用PyTorch默认的并行计算调度方式（即不另外实现任何调度算法），对于串行连接的模型架构而言，模型的结构较深，因此数据流的传播速度较慢，推理速度较慢，而对于并行连接的模型而言，多个任务的推理过程在编码器可以同时进行，推理时间少于串行连接的模型。

== 消融实验

本节主要对本文搭建的PSRec模型进行模块层面和训练方式层面的消融实验。在特征增强模块中，本节将讨论SRB模块的个数对图像重建效果和识别效果的影响。在训练方式中，本节将讨论超分损失的组成方式以及是否使用多任务损失权重对模型在TextZoom测试集上性能的影响。

*SRB数目的影响*。特征增强模块中SRB个数对模型推理性能有一定的影响。SRB是特征增强模块的主要组成部分，而特征增强模块的输出特征会被识别器和超分器共享，不仅需要加强编码器的输出特征，还需要进一步融合文本特征和超分特征，因此有必要探究SRB模块对整个模型的性能的影响。主要的比较指标包括TextZoom测试集上体现对图像的重建能力的PSNR和SSIM，以及识别器的识别准确率，在主要指标之外，本节还讨论了使用不同数量SRB的PSRec模型的单位参数准确率和PSNR，以此作为模型的选择标准。不同数量的SRB模块对整个模型性能的影响结果如@tab4-4a 和@tab4-4b 所示，其中@tab4-4a 展现了模型的超分辨率性能，而@tab4-4b 展现了模型的识别性能。

#figure(
  tablex(
    auto-lines:false,
    columns:cycle((1fr,), 9),
    rows:2em,
    stroke:1pt,
    repeat-header:true, //换页时重复表头
    header-rows:2, //表头行数
    align:center+horizon,
    hlinex(),
    [],colspanx(4)[PSNR],(),(),(),colspanx(4)[SSIM],(),(),(),
    hlinex(),
    [$N_("SRB")$],[E],[M],[H],[*A*],[E],[M],[H],[*A*],
    hlinex(),
    [0],[23.50], [19.86], [20.59], [21.43], [0.7817], [0.5241], [0.6172], [0.6480],
    [1],[23.85], [19.85], [20.81], [21.62], [0.7916], [0.5271], [0.6207], [0.6537],
    [2],[23.52], [19.97], [20.53], [21.45], [0.7748], [0.5183], [0.6060], [0.6401],
    [3],[24.05], [*20.01*], [20.87], [21.77], [0.8049], [0.5377], [0.6334], [0.6660],
    [4],[*24.29*], [19.93], [*20.98*], [*21.86*], [*0.8161*], [*0.5414*], [*0.6397*], [*0.6732*],
    hlinex(),
  ),
  kind:table,
  caption: [使用不同数目的SRB（$N_("SRB")$）对PSRec模型在TextZoom测试集上超分辨率性能的影响。其中E表示Easy简单测试集，M表示Medium中等测试集，H表示Hard困难测试集，A表示三个测试集的加权平均。]
)<tab4-4a>

#figure(
  tablex(
    auto-lines:false,
    columns:cycle((1fr,),5),
    rows:2em,
    stroke:1pt,
    repeat-header:true, //换页时重复表头
    header-rows:1, //表头行数
    align:center+horizon,
    hlinex(),
    [$N_("SRB")$],[E],[M],[H],[*A*],
    hlinex(),
    [0], [0.6677], [0.5106], [0.3768], [0.5276],
    [1], [0.6485], [0.5355], [0.3708], [0.5227],
    [2], [0.6547], [0.5050], [0.3604], [0.5159],
    [3], [*0.7172*], [0.5447], [*0.3887*], [*0.5605*],
    [4], [0.6974], [*0.5532*], [0.3797], [0.5532],
    hlinex(),
  ),
  kind:table,
  caption: [使用不同数目的SRB（$N_("SRB")$）对PSRec模型在TextZoom测试集上识别准确率的影响。其中E表示Easy简单测试集，M表示Medium中等测试集，H表示Hard困难测试集，A表示三个测试集的加权平均。]
)<tab4-4b>

如@tab4-4a 所示，总体而言，SRB的数目越多，模型的超分性能越好。在特征增强模块中使用$4$个SRB的效果最好，在Easy测试集上的PSNR达到了$24.29$，并且三个测试集的平均PSNR也更加接近$22.0$。但对于使用$3$个SRB模块的模型而言，其性能与使用更多的SRB模块差距并不大，甚至在Medium测试集上的PSNR超过了$4$个SRB的模型，表明在一定的范围内，增加SRB的数量可以增强模型的超分性能。而仅使用编码器进行特征编码时的PSNR最低，但总体而言相差不大，例如其在Easy测试集上的PSNR比使用$4$个SRB模块的模型只低了$0.79$，表明使用高分辨率降采样图像预训练的参数初始化模型，后续在低分辨率图像上进行微调的方式可以带来较高的模型鲁棒性。

而在准确率方面，结论与超分任务的结论略有不同。@tab4-4b 展现了使用不同数目的SRB对模型在不同测试集上的识别准确率的影响。从表中可以看出，在PSRec的特征增强模块中使用$4$个SRB时的识别准确率低于使用$3$个SRB，表明增加SRB在一定范围内对于识别任务的性能有一定的提升，但过多的SRB会造成性能的饱和。

对于拥有多任务评估结果的PSRec模型，本文还探究了增加SRB个数在模型的参数量以及运算能力的影响。如@tab4-4c 所示，在特征增强模块上使用$3$个SRB能够带来最高的单位参数准确率，即平均每$1$M参数可以贡献$0.047$准确率。而对于PSNR而言，特征增强模块中不使用任何SRB模块可以达到最高的单位参数PSNR，平均每$1$M参数可以提升$1.8492$的PSNR指标，但由@tab4-4a 和@tab4-4b 发现，不使用任何SRB模块的模型的整体性能远低于使用$3$个SRB的模型性能，因此本文最终选择增强模块中带有$3$个SRB的模型进行实验。

#figure(
  tablex(
    auto-lines:false,
    columns:(1fr,1.5fr,1.5fr,1.5fr,2fr,2fr),
    rows:2em,
    stroke:1pt,
    repeat-header:true, //换页时重复表头
    header-rows:2, //表头行数
    align:center+horizon,
    hlinex(),
    [$N_("SRB")$], [FPS(Hz)], [Params(M)], [FLOPs(G)], [PSNR/Params(M)], [ACC/Params(M)],
    hlinex(),
    [0], [186.85], [11.59], [1.50], [*1.8492*], [0.0458],
    [1], [153.13], [11.71], [1.63], [1.8466], [0.0465],
    [2], [125.95], [11.83], [1.75], [1.8134], [0.0456],
    [3], [108.87], [11.95], [1.88], [1.8216], [*0.0470*],
    [4], [98.43 ], [12.07], [2.00], [1.8114], [0.0464],
    hlinex(),
  ),
  kind:table,
  caption: [使用不同数目的SRB模块对模型规模和推理速度的影响。]
)<tab4-4c>

*梯度剖面损失*。超分辨率任务使用不同权重的梯度剖面损失能够给模型性能带来不同程度的影响。@tab4-5a 展示了使用和不使用梯度剖面损失以及不同权重的梯度剖面损失对模型在TextZoom测试集上的性能影响。从表中可以发现，当$lambda_2=1 times 10^(-4)$时，训练的模型性能相比于其它权重更好。在三种难度测试集的PSNR都超过了$20$，并且在简单测试集的PSNR超过了$24$。除此之外，当$lambda_2=0.5 times 10^(-4)$时，训练出的模型超分性能依然较好，并在中等难度的测试集上的PSNR和SSIM超过了$lambda_2=1 times 10^(-4)$的模型性能。因而在超分任务中加入适当的梯度剖面损失，为模型提供图像锐度信息，可以有效提高模型的超分辨率性能。

//TODO: 可以展示图片
// 为了更加具体地展现梯度剖面损失对图像超分辨率重建的效果，\@展示了对于同一张文本图像的超分辨率重建效果。

#figure(
  placement: auto,
  tablex(
    auto-lines:false,
    columns:(1fr,0.6fr,0.6fr,0.6fr,0.6fr,0.5fr,0.5fr,0.5fr,0.5fr,),
    rows:2em,
    stroke:1pt,
    repeat-header:true, //换页时重复表头
    header-rows:2, //表头行数
    align:center+horizon,
    hlinex(),
    [],colspanx(4)[PSNR],(),(),(),colspanx(4)[SSIM],(),(),(),
    hlinex(),
    [$lambda_2(times 10^(-4))$],[E],[M],[H],[*A*],[E],[M],[H],[*A*],
    hlinex(),
    [0],   [21.46], [19.26], [19.57], [20.17], [0.7301], [0.4960], [0.5661], [0.6041],
    [0.5], [23.86], [*20.04*], [20.84], [21.70], [0.8041], [0.5373], [*0.6344*], [0.6658],
    [1],   [*24.05*], [20.01], [*20.87*], [*21.77*], [*0.8049*], [*0.5377*], [0.6334], [*0.6660*],
    [5],   [23.81], [19.98], [*20.87*], [21.67], [0.8009], [0.5332], [0.6325], [0.6627],
    hlinex(),
  ),
  kind:table,
  caption: [使用不同权重$lambda_2$的梯度剖面损失$L_("GP")$对模型超分辨率性能的影响。其中超分损失中$L_2$的权重为固定为$20$。E表示Easy简单测试集，M表示Medium中等测试集，H表示Hard困难测试集，A表示三个测试集的加权平均。]
)<tab4-5a>

#figure(
  tablex(
    auto-lines:false,
    columns:(1fr,0.6fr,0.6fr,0.6fr,0.6fr,),
    rows:2em,
    stroke:1pt,
    repeat-header:true, //换页时重复表头
    header-rows:1, //表头行数
    align:center+horizon,
    hlinex(),
    [$lambda_2(times 10^(-4))$],[E],[M],[H],[*A*],
    hlinex(),
    [0],    [0.0006], [0.0   ], [0.0   ], [0.0002],
    [0.5],  [0.6714], [0.5440], [0.3872], [0.5429],
    [1],    [*0.7172*], [0.5447], [0.3753], [*0.5605*],
    [5],    [0.6980], [*0.5496*], [*0.3902*], [0.5555],
    hlinex(),
  ),
  kind:table,
  caption: [使用不同权重$lambda_2$的梯度剖面损失$L_("GP")$对模型识别准确率的影响。其中超分损失中$L_2$的权重为固定为$20$。E表示Easy简单测试集，M表示Medium中等测试集，H表示Hard困难测试集，A表示三个测试集的加权平均。]
)<tab4-5b>

*不确定性权重*。在多任务学习中，使用可学习的权重参数训练模型可以给模型带来一定的性能提升。如@tab4-6 所示，在训练使用多任务损失函数训练模型时，将损失函数的权重设置为可学习参数的一部分，可以在一定程度上提高模型的超分辨率和识别性能。其中，使用了可学习损失权重的PSRec模型在PSNR上达到了最高水平，即$21.77$，并由@tab4-1 所示，使用该损失训练的PSRec超过了TSRN模型的超分辨率性能。在准确率方面，使用可学习的权重训练的模型的平均识别准确率，比未使用可学习权重训练的模型高了$2%$。因此可以看出对于多任务学习，不同任务之间的损失权重设置会对模型性能带来一定影响。

#figure(
  grid(
    columns: 2,
    gutter: 1%,
    [#subfigure(
      image("fig/weight_ratio_dense.svg"),
      caption: [使用dense方式进行微调的模型]
    )<fig4-weight-a>],
    [#subfigure(
      image("fig/weight_ratio_mlp.svg"),
      caption: [使用mlp方式进行微调的模型]
    )<fig4-weight-b>],
    [#subfigure(
      image("fig/weight_ratio_rnn.svg"),
      caption: [使用rnn方式进行微调的模型]
    )<fig4-weight-c>],
    [#subfigure(
      image("fig/weight_ratio_rnn_mlp.svg"),
      caption: [使用rnn-mlp方式微调的模型]
    )<fig4-weight-c>],
  ),
  caption: [训练过程中不确定性加权对不同微调方式模型的影响。其中绿色线条表示超分辨率任务损失权重中的参数$sigma_s$，紫色线条表示识别任务损失权重中的参数$sigma_r$，灰色虚线表示两个参数的比值。]
)<fig4-weight>

#figure(
  tablex(
    auto-lines:false,
    columns:(3fr,2fr,2fr,2fr),
    rows:2em,
    stroke:1pt,
    repeat-header:true, //换页时重复表头
    header-rows:1, //表头行数
    align:center+horizon,
    hlinex(),
    [是否使用可学习的损失权重],[PSNR],[SSIM],[ACC],
    hlinex(),
    [$times$],     [21.52], [0.6628], [0.5397],
    [$checkmark$], [*21.77*], [*0.6660*], [*0.5605*],
    hlinex(),
  ),
  kind:table,
  caption: [使用可学习的损失权重对模型性能的影响。三个指标分别表示在TextZoom的三个测试集上的加权平均值。]
)<tab4-6>

为了展示不确定性权重对训练过程中实际的调控过程，@fig4-weight 展示了不同微调方式的模型在使用了不确定性加权后的各个任务对应损失函数的权重，以及任务之间的比值随着训练过程的变化。值得注意的是，图中展示的曲线并非实际的损失权重，其关系式由@eq3-weightedloss 给出。总体而言，二者的权重参数$sigma_s,sigma_r$呈现下降趋势，表明随着训练的进行，超分辨率任务的损失和识别任务的损失分别在下降，因而需要加大权重。对于由灰色虚线所代表的权重参数比值而言，表明随着训练的进行，两个任务的权重并非一成不变，而是需要不断地调整。由图中可以看出，识别损失的权重参数大于超分损失，因而其权重小于超分损失，而这是可学习的权重参数在模型训练时，根据识别损失的值大于超分损失的值自行调控的结果。

//TODO:
== 推理结果
本节主要展示不同模型的推理可视化结果，并分析不同模型在图像超分辨率和识别任务上结果的异同。

#figure(
  image("fig/psrec_infer.svg", width: 85%),
  caption: [PSRec模型推理结果可视化]
)<fig4-psrec-infer>

@fig4-psrec-infer 展示的为PSRec模型在TextZoom测试集上的部分推理可视化结果。每组图片最左边为低分辨率LR图像，最右边为HR高分辨率图像，中间为PSRec模型超分辨率SR图像。在SR图像下方文本为PSRec模型识别结果，红色代表识别错误，HR图像下方文本为真实文本标签。总体而言，PSRec可以做到将LR图像中由于模糊而紧挨着的文本分割开来，例如LR图像中的“sick”，其中“s”与“i”在LR图像中非常靠近，但经过PSRec处理后，输出的SR图像能最大程度将“s”和“i”分割开，与之类似的还有“plan”图像，其SR图像同样能做到最大程度分隔“p”和“l”两个字符。除此之外对于LR图像中难以辨认文本的情况，例如“while”，PSRec模型同样可以凭借带有文本信息的特征增强模块进行超分辨率，将LR图像中模糊的“il”重建为SR图像，并识别出正确的文本。但对于一些自行十分相近的字符，例如“i”和“l”，若无明显图像特征，PSRec则会将其混淆，例如“weibo”中将“i”识别为“l”，但在“indicate”中将“i”正确识别。

#figure(
  image("fig/tsrn_psrec.svg", width: 85%),
  caption: [PSRec模型和TSRN+CRNN模型在超分辨率推理效果和识别文本识别上的对比。红色文本表示识别错误。红色框内表示重建效果对比明显的区域]
)<fig4-tsrn-psrec>

并行任务结构的模型PSRec相较于TSRN+CRNN串行连接的模型，@fig4-tsrn-psrec 展示了较为细致的对比。从图中可以发现，TSRN的超分辨率效果在一定程度上会出现错误，例如“early”中的第一个字符“e”，TSRN完全将其重建为另一个字符“c”，从而进一步导致了后续识别的错误。除此之外还有“feng”，在LR图像中的“e”字符中心存在一些笔画，但从TSRN的重建效果来看，这些微弱的笔画被当作文本背景或其它噪声，导致其重建的SR图像中将“e”重建为“o”或“a”，识别器受到SR图像的影响，因此识别出错误的文本。不仅如此，PSRec的重建可以保持原有字符笔画的连续性，例如“seen”，其中字符“n”的左右通过连续的笔画连接，通过PSRec超分辨率后的图像二者同样保持连续的笔画连接，但经过TSRN处理后，字符“n”的左右发生了断裂，并进一步误导了识别器识别结果。

= 总结与展望

本文主要搭建了一种并行连接超分器和识别器结构的多任务模型PSRec。该模型分为编码器，特征增强模块、识别解码器以及超分解码器。其中不同任务的解码器共用编码器特征。在输入低分辨率图像时，图像先带有高分辨率信息的编码器进行特征的初步处理，再经过特征增强模块完成超分辨率特征和文本特征的融合，并输出融合后的特征，接着识别解码器和超分解码器分别将该特征作为输入，通过损失函数进行监督训练。在编码器和特征增强模块中，PSRec模型使用SRB作为主干，其中的BiGRU结构较好地融合了图像特征与文本特征。除此之外，PSRec使用两个任务的损失函数进行训练，并在各损失之间使用不确定性加权来根据训练过程动态平衡不同任务的权重。

在实验中，PSRec在超分辨率性能和识别性能中都达到了较好的指标，其中最高的平均PSNR达到了$21.77$，而最高的识别准确率达到了$56.05%$，比相同配置下的其它模型性能更高。而相比于串行方式连接的模型TSRN+CRNN，PSRec并行连接超分器和识别器的优势体现在推理速度快、参数量小等方面。不仅如此，在消融实验中，本文探究了SRB数目、超分辨率损失的组成形式和不确定性加权与模型性能的关系，并认为SRB在一定范围内可以同时增强模型的超分辨率性能和识别性能，若使用恰当的梯度剖面损失可以给模型带来超分辨率上的提升，而使用可学习的权重参数代替固定权重可以对多任务学习效果来带提升。最后本文通过推理结果的对比，体现出PSRec模型在图像重建和识别准确率上的优势，为该领域后续的工作打下了基础。

通过实验，本文发现PSRec在SSIM指标上仍然存在不足，即使能较好地恢复低分辨率图像，但仍会出现文本字形的扭曲和图像色泽的损失。因此在超分辨率任务中要同时达到高指标的PSNR和SSIM仍然是一项具有挑战的课题。

#seu_bibliography("../ref.bib")

#appendix[
= 损失函数

== 识别损失

设输入CTC损失的特征图 $F$ 的宽为 $T$（即时间序列长度），目标单词表大小为 $S_c$，则分类的类别一共有 $S=S_c+1$ 类（包括空白字符 $epsilon$ ）。CTC损失的标签对齐方式为：对于特征图 $F in bb(R)^(T times S)$，按照时间步顺序依次预测字符，若预测出的字符长度满足空白字符个数与单词表字符个数和为 $T$，且单词表字符必须大于 $0$，即 

$
T_epsilon + T_c = T, T_epsilon >= 0, T_c > 0
$<eqA-ctc>

#figure(
  image("fig/markov.svg", width: 75%),
  caption: [CTC损失对应的隐马尔可夫链]
)<figA-ctc>

则该预测标签为一个合法的对齐方式，设所有合法的对齐标签为集合 $cal(S)_a$。CTC损失满足如@figA-ctc 所示的隐马尔可夫过程：包括空白字符在内的字符表示一个隐状态，约束条件由@eqA-ctc 给出。设从预测开始，到时间步 $i$ 时，已经产生 $j$ 个单词表字符的概率为前向概率 $alpha_(i,j)$，则由马尔可夫性，

$
alpha_(i,j) = alpha_(i-1,j) p_(i-1,j)(epsilon) + alpha_(i,j-1) p_(i,j-1)(c_j)
$<eqA-alpha>

其中 $p_(i,j-1)(c_j)$ 表示在时间步为 $i$ 预测为第 $j$ 个单词表字符的概率，$p_(i-1,j)(epsilon)$ 表示在时间步为 $i-1$ 预测出空白字符的概率，显然由递归关系可以得到@eqA-alpha。同理，设从时间步 $i$，位于第 $j$ 个单词表字符开始，从 $i+1$ 到 $T$ 会产生合法对齐标签的概率为后向概率 $beta_(i,j)$，则由马尔可夫性，

$
beta_(i,j) = beta_(i+1,j) p_(i,j) (epsilon) + beta_(i,j+1) p_(i,j) (c_(j+1))
$

因此给定输入特征，预测标签的概率为

$
p_theta (L_p | F) = sum_(a in cal(S)_a) p_theta (a | F)
$

其中 $p_theta (a | F) = Pi_(p in "path") p$ 表示一个合法对齐标签路径上所有节点概率的乘积，整个标签预测的概率即所有合法路径概率的和。每一个节点概率 $p in "path"$ 是由模型网络产生的。以在时间步 $i$ 已经预测出 $j$ 个字符，且下一个预测字符为 $c$ 的节点概率 $p_(i,j)(c)$ 为例，在反向传播时，由链式法则得到

$
(partial p_theta (L_p | F)) / (partial theta) = sum_(a in cal(S)_a) (partial p_(i,j)(c)) / (partial theta) (partial p_theta (L_p | F)) / (partial p_(i,j)(c))
$

$(partial p_(i,j)(c)) / (partial theta)$ 与整个模型有关，后一项 $(partial p_theta (L_p | F)) / (partial p_(i,j)(c))$ 由CTC损失路径确定。将所有路径 $cal(S)_a$ 分为包含节点 $p_(i,j)(c)$ 的部分 $cal(S)_a^i$ 和不包含节点 $p_(i,j)(c)$ 的部分$cal(S)_a^e$，则对 $p_(i,j)(c)$ 求偏导后，与其无关的路径导数为 $0$，其余部分可以写为

$
(partial p_theta (L_p | F)) / (partial p_(i,j)(c)) = sum_(a in cal(S)_a^i) (p_theta (a | F)) / (p_(i,j)(c)) = 1 / (p_(i,j)(c)) sum_(a in cal(S)_a^i) p_theta (a | F)
$

求和号中的式子表示在时间步 $i$ 时，已经产生 $j$ 个单词表字符、且在当前状态产生下一个单词表字符为 $c$、且从 $i+1$ 到 $T$ 将产生 $T-i$ 会产生合法对齐标签的概率，即在时间步为 $i$、已经产生 $j$ 个单词表字符的前向概率、转移概率和后向概率之积：

$
1  / (p_(i,j)(c)) sum_(a in cal(S)_a^i) (p_theta (a | F)) = 1  / (p_(i,j)(c)) alpha_(i,j) p_(i,j) (c) beta_(i,(j+1)) = alpha_(i,j) beta_(i,(j+1))
$

即最终CTC损失反向传播时更新的梯度为

$
g_t = (partial P_theta_t (hat(Y) | X)) / (partial theta_t) = sum_(i,j,c) (partial p_(i,j)(c)) / (partial theta_t) alpha_(i,j) beta_(i,j+1)
$

== 多任务损失

一般而言，识别任务为分类任务，超分任务为回归任务，其似然函数可以分别表示为

$
p_r (cal(F)_theta (I_l)=c | I_l) = "softmax" (1 / sigma_r^2 cal(F)_theta (I_l))\
p_s (I_s | I_l,I_h) = cal(N) (cal(F)_theta (I_l), sigma_s)
$<eqA-p>

其中 $sigma_r>0, sigma_s>0$ 分别表示识别任务和超分辨率任务中的不确定性（识别任务表达式类似于玻尔兹曼分布，则该不确定性参数表示分布中的温度；而超分任务表达式中不确定性参数表示方差）。模型训练的目的即通过优化参数 $theta$ 最大化对数似然分布

$
theta^star &= "argmax"_theta log p(c, I_s | I_l, I_h)\
&= "argmax"_theta log p_r (c | I_l) + log p_s (I_s | I_l)
$

其中 $p,p_r,p_s$ 都是带有模型参数 $theta$ 的似然函数。对于识别任务 $log p_r (c | I_l)$，由@eqA-p 可以写为

$
log p_r (cal(F)_theta (I_l)=c | I_l) &= log "softmax" (1 / sigma_r^2 cal(F)_theta (I_l))\
&= log (exp (1/sigma_r^2 cal(F)_theta (I_l))) / (sum_c exp (1/sigma_r^2 cal(F)_theta (I_l)))\
&= 1/sigma_r^2 cal(F)_theta (I_l) - log sum_c exp (1/sigma_r^2 cal(F)_theta (I_l))
$

而对于超分任务 $log p_s (I_s | I_l)$ 可以写为

$
log p_s (I_s | I_l,I_h) &= log cal(N) (cal(F)_theta (I_l), sigma_s)\
&prop -1/(2 sigma_s^2) || I_h - cal(F)_theta (I_l) ||^2 - log sigma_s
$

损失函数定义为最小化形式，可以表示为（近似过程参考文献~@uncertainty-loss）

$
cal(L) &= -log p(c,I_s | I_l,I_h)\
&= 1/(2 sigma_s^2) || I_h - cal(F)_theta (I_l) ||^2 + log sigma_s - 1/sigma_r^2 cal(F)_theta (I_l) + log sum_c exp (1/sigma_r^2 cal(F)_theta (I_l))\
&approx 1/(2 sigma_s^2) cal(L)_s + 1/(sigma_r^2) cal(L)_r + log sigma_s + log sigma_r
$

为了防止正则项出现负数，同时方便参数学习，可以采用如下方式~@uncertainty-loss-2 进行损失的计算

$
cal(L) = 1/(2 sigma_s^2) cal(L)_s + 1/(2 sigma_r^2) cal(L)_r + log (1 + sigma_s^2) + log (1 + sigma_r^2)
$<eq-loss>

本文模型最终的多任务损失函数即由~@eq-loss 给出。其中 $sigma_r,sigma_s$可以在训练前指定特定值，也将其加入模型中的可学习的参数中，随着模型参数一同更新，实现动态调整梯度传播时的权重。

= 模型性能

== 最优模型

本文实现的PSRec模型为多任务模型，因此对于每个任务而言，在模型训练时都分别保存不同任务的最优模型，即最优准确率模型和最优PSNR模型。在正文中，若无特殊说明，一般使用的是最优PSNR模型。本附录将进行这两类最优模型之间的对比。

@tabB-avgloss-a 和@tabB-avgloss-b 展示了用固定权重的损失@eq3-avgloss 训练的模型分别使用最优准确率参数和最优PSNR参数的推理性能。而@tabB-wloss-a 和@tabB-wloss-b 展示了用不确定性权重参数@eq3-weightedloss 训练的模型分别使用最优准确率和最优PSNR参数的推理性能。

#figure(
  tablex(
    auto-lines:false,
    columns:(2fr,1fr,1fr,1fr,1fr,1fr,1fr,1fr,1fr,1fr,),
    rows:2em,
    stroke:1pt,
    repeat-header:true,
    header-rows:2,
    align:center+horizon,
    hlinex(),
    [],[],colspanx(4)[PSNR],(),(),(),colspanx(4)[SSIM],(),(),(),
    hlinex(),
    [微调策略],[参数],[E],[M],[H],[*A*],[E],[M],[H],[*A*],
    hlinex(),
    [PSRec-dense],   [A], [22.86],[19.31],[20.66],[21.04],[0.8078],[0.5359],[0.6372],[0.6677],
    [PSRec-mlp],     [A], [23.69],[19.97],[20.63],[21.55],[0.7963],[0.5318],[0.6195],[0.6566],
    [PSRec-rnn],     [A], [23.42],[19.49],[20.76],[21.34],[0.8100],[0.5339],[0.6369],[0.6676],
    [PSRec-rnn-mlp], [A], [22.89],[19.66],[20.57],[21.14],[0.7866],[0.5272],[0.6122],[0.6493],
    [PSRec-dense],   [P], [23.40],[19.65],[20.72],[21.36],[0.8004],[0.5333],[0.6289],[0.6615],
    [PSRec-mlp],     [P], [23.98],[19.92],[20.93],[21.73],[0.8101],[0.5385],[0.6370],[0.6692],
    [PSRec-rnn],     [P], [23.41],[19.61],[20.79],[21.38],[0.8086],[0.5362],[0.6374],[0.6680],
    [PSRec-rnn-mlp], [P], [23.66],[19.78],[20.77],[21.52],[0.8029],[0.5336],[0.6297],[0.6628],
    hlinex()
  ),
  kind:table,
  caption: [使用固定加权损失方式训练的模型在TextZoom数据集上三种难度测试集的超分辨率性能。模型后的“-”表示使用的微调策略。参数类型中“A”表示使用最优准确率模型参数，“P”表示使用最优PSNR模型参数]
)<tabB-avgloss-a>

#figure(
  tablex(
    auto-lines:false,
    columns:(2fr,1fr,1fr,1fr,1fr,1fr),
    rows:2em,
    stroke:1pt,
    repeat-header:true, //换页时重复表头
    header-rows:1, //表头行数
    align:center+horizon,
    hlinex(),
    [微调策略],[参数类型], [Easy],[Medium],[Hard],[*Avg*],
    hlinex(),
    [PSRec-dense],  [A], [0.7067], [0.5446], [0.3961], [0.5589],
    [PSRec-mlp],    [A], [0.6918], [0.5461], [0.3961], [0.5539],
    [PSRec-rnn],    [A], [0.7054], [0.5581], [0.3917], [0.5615],
    [PSRec-rnn-mlp],[A], [0.6900], [0.5567], [0.3969], [0.5569],
    [PSRec-dense],  [P], [0.6775], [0.5319], [0.3968], [0.5443],
    [PSRec-mlp],    [P], [0.6825], [0.5468], [0.3946], [0.5502],
    [PSRec-rnn],    [P], [0.6986], [0.5426], [0.3931], [0.5544],
    [PSRec-rnn-mlp],[P], [0.6770], [0.5298], [0.3850], [0.5397],
    hlinex()
  ),
  kind:table,
  caption: [使用固定加权损失方式训练的模型在TextZoom数据集上三种难度测试集的识别准确率。模型后的“-”表示使用的微调策略。参数类型中“A”表示使用最优准确率模型参数，“P”表示使用最优PSNR模型参数]
)<tabB-avgloss-b>

#figure(
  tablex(
    auto-lines:false,
    columns:(2fr,1fr,1fr,1fr,1fr,1fr,1fr,1fr,1fr,1fr,),
    rows:2em,
    stroke:1pt,
    repeat-header:true,
    header-rows:2,
    align:center+horizon,
    hlinex(),
    [],[],colspanx(4)[PSNR],(),(),(),colspanx(4)[SSIM],(),(),(),
    hlinex(),
    [微调策略],[参数],[E],[M],[H],[*A*],[E],[M],[H],[*A*],
    hlinex(),
    [PSRec-dense],   [A],[23.56],[19.52],[20.76],[21.39],[0.8159],[0.5390],[0.6431],[0.6734],
    [PSRec-mlp],     [A],[23.67],[19.82],[20.84],[21.56],[0.7980],[0.5350],[0.6299],[0.6615],
    [PSRec-rnn],     [A],[23.05],[19.64],[20.78],[21.25],[0.8043],[0.5359],[0.6351],[0.6657],
    [PSRec-rnn-mlp], [A],[24.05],[20.01],[20.87],[21.77],[0.8049],[0.5377],[0.6334],[0.6660],
    [PSRec-dense],   [P],[23.50],[19.70],[20.78],[21.44],[0.8047],[0.5382],[0.6322],[0.6657],
    [PSRec-mlp],     [P],[23.67],[19.82],[20.84],[21.56],[0.7980],[0.5350],[0.6299],[0.6615],
    [PSRec-rnn],     [P],[23.54],[19.86],[20.74],[21.49],[0.8061],[0.5362],[0.6328],[0.6657],
    [PSRec-rnn-mlp], [P],[24.05],[20.01],[20.87],[21.77],[0.8049],[0.5377],[0.6334],[0.6660],
    hlinex()
  ),
  kind:table,
  caption: [使用可学习权重损失方式训练的模型在TextZoom数据集上三种难度测试集的超分辨率性能。模型后的“-”表示使用的微调策略。参数类型中“A”表示使用最优准确率模型参数，“P”表示使用最优PSNR模型参数]
)<tabB-wloss-a>

#figure(
  tablex(
    auto-lines:false,
    columns:(2fr,1fr,1fr,1fr,1fr,1fr),
    rows:2em,
    stroke:1pt,
    repeat-header:true, //换页时重复表头
    header-rows:1, //表头行数
    align:center+horizon,
    hlinex(),
    [微调策略],[参数类型], [Easy],[Medium],[Hard],[*Avg*],
    hlinex(),
    [PSRec-dense],  [A],[0.7197],[0.5532],[0.4028],[0.5686],
    [PSRec-mlp],    [A],[0.6906],[0.5418],[0.3939],[0.5514],
    [PSRec-rnn],    [A],[0.7110],[0.5667],[0.3931],[0.5667],
    [PSRec-rnn-mlp],[A],[0.7172],[0.5447],[0.3887],[0.5605],
    [PSRec-dense],  [P],[0.7079],[0.5532],[0.3879],[0.5596],
    [PSRec-mlp],    [P],[0.6906],[0.5418],[0.3939],[0.5514],
    [PSRec-rnn],    [P],[0.7004],[0.5631],[0.3842],[0.5589],
    [PSRec-rnn-mlp],[P],[0.7172],[0.5447],[0.3887],[0.5605],
    hlinex()
  ),
  kind:table,
  caption: [使用使用可学习权重损失方式训练的模型在TextZoom数据集上三种难度测试集的识别准确率。模型后的“-”表示使用的微调策略。参数类型中“A”表示使用最优准确率模型参数，“P”表示使用最优PSNR模型参数]
)<tabB-wloss-b>


]

#acknowledgment[

论文写就，立夏已过。若无他助，难成本文。笔者思索，在此为谢。

首先，感谢我的导师薛晖教授，是她带给了我模式识别课程上的启蒙，带领我走进人工智能领域。若没有她的教导，我或许无法完成此文。接着要感谢同组的师兄师姐，学习路上若无你们给我树立的榜样，我或许无法完成此文。此外，特别感谢朱士鹏师兄对本文的悉心指导，并为本文的实验提供了充足的算力支持。另外，感谢吴健雄学院提供的优质资源，为我的求学生活提供了良好的学习环境。感谢健雄书院提供的多彩活动和人文理念，让我成为管理团队的一员，帮助我在心理上获得成长。

其次，感谢本科四年间所有在成长和学习上帮助过我的朋友和老师。感谢辅导员菜菜对我大三大四生活的关照。感谢瓜群和立交桥中的每一位成员，你们为我的本科生活带来了众多难忘的瞬间。感谢十只鸡，Jerry，然然，PC，T老师等学长学姐为我的成长和大学生活带来诸多快乐和思考，你们永远是我的榜样。感谢Zan，JenniferWu，Henny，421，TtuHamg，King，Pepper，SYM，LYL等朋友为我的本科生活带来了风、故事、大海和岛屿，没有你们的陪伴，我或许无法体验到真正的大学生活。

除此之外，感谢一直努力的自己，在幽暗昏惑之时依然能坚持。

最后，特别感谢一路以来一直陪伴着我的父母，在我缺席本应陪伴他们的无数的时光里，他们一如既往地给予我无限的支持、关心和爱。正是他们对生活的热爱和我对的照顾，让我能幸福成长并顺利完成学业。

感谢大学期间的所有相遇，与一草一木，与一朝一夕。

]