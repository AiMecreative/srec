#import "includes/template.typ": bachelor_conf, seu_bibliography
#import "includes/template.typ": acknowledgment, appendix
#import "includes/utils.typ": set_doc_footnote, subfigure
#import "@preview/funarray:0.3.0":cycle
#import "@preview/tablex:0.0.8": tablex,vlinex,hlinex,colspanx,rowspanx
#show :doc => set_doc_footnote(doc)


#let info = (
  title: [面向低分辨率场景文本图像的增强与识别技术研究],
  student_id: [61520324],
  name: [许睿],
  college: [吴健雄学院],
  major: [计算机科学与技术],
  supervisor: [薛晖],
  duration: "2023.12~2024.6",

  zh_abstract: [
    中文摘要
  ],

  zh_key_words: (
    "关键字1",
    "关键字2"
  ),

  en_abstract: [
    #lorem(250)
  ],

  en_key_words: (
    "Keywords1",
    "Keywords2"
  )
)


#show :doc => bachelor_conf(
  doc,
  ..info
)

#figure(
  tablex(
    auto-lines: false,
    columns: cycle((1fr, 2fr, 1fr), 3),
    stroke: 1pt,
    align: left+horizon,

    hlinex(),
    [术语], [英文], [中文],
    hlinex(),
    [STR], [Scene-Text Recognition], [场景文本识别],
    [STISR], [Scene-Text Image Super-Resolution], [场景文本图片超分],
    [NLP], [Natural Language Processing], [自然语言处理],
    [OCR], [Optical Character Recognition], [光学字符识别],
    [LR], [Low Resolution], [低分辨率],
    [HR], [High Resolution], [高分辨率],
    [SR], [Super-Resolution], [超分辨率],
    [PSNR], [Peak Signal Noise Ratio], [峰值信噪比],
    [SSIM], [Structural Similarity Index Measure], [结构一致性指标],
    [SRB], [Sequential Residual Block], [序列残差块],
    hlinex()
  ),
  kind: table,
  caption: [中英术语对照表]
)

= 绪论

== 课题背景和意义

//TODO: 补充图片，说明SITSR, STR
// draw image:
// LR image
// SR image
// HR image
从一般性的场景图片中识别文本信息不仅能帮助深度学习模型在训练时理解场景逻辑，还能在推理时给予使用者更多场景相关的信息，该类型的任务被称为场景文本识别（Scene-Text Recognition, STR）。
场景文本识别能在诸多实际应用中得到重要应用：例如在辅助驾驶中，汽车可以动态识别路上的交通符号，以便根据路况和交通规则做出安全的决策。
近年来有许多研究人员躬身场景文本识别领域并取得了很多优秀成果。这些模型在目前广泛使用的基准数据集 ~@cute80~@icdar2013~@coco-text 中对于高分辨率（High-Resolution, HR）的图像取得了非常高的准确率。
现实中的STR任务比基准数据集的形式更为复杂，被识别的字形可能因设计要求等因素，单个字符可以有多种字体和不同程度的扭曲，因此也有很多研究人员提出可以使用矫正模块 @aster 对场景文本进行空间上的矫正，使其便于识别。因此给一张比较清晰的场景文本图像，当前有许多方式可以以较高置信度对其进行识别。

然而，在场景文本图像中的特征受到大量影响时，识别器的性能也大幅下降。例如，受到拍摄环境或拍摄器材的制约时，拍摄的场景文本图像可能带有一定的模糊，这对识别器的鲁棒性带来了巨大的挑战。
因此有研究人员 @tsrn 提出了更加一般的STR任务，即对于带有模糊等不良因素的低分辨率（Low-Resolution, LR）图像，可以先进行场景文本图像超分辨率（Scene-Text Image Super-Resolution, STISR）处理，得到较为清晰的超分辨率（Super-Resolution, SR）图像后，再进行常规的STR。该研究人员构建了专门的模糊场景文本数据集 @tsrn 用于评判模型在不良条件下的超分辨率和识别能力。与传统的图像超分辨率（Image Super-Resolution）任务不同，STISR任务更加注重于文本的恢复，由于不少文本包含一定的语义信息，这不仅给图像超分辨率模型带来了挑战，也使STISR模型得到更多新的思路。

对于LR场景文本的识别，当前广泛使用的模型架构是如@fig1-1a 所示的串行架构。这种架构虽然直观，但同时存在不少问题。串行架构中，两个任务的耦合程度太高，STR任务非常依赖于STISR任务的性能，如果STISR得到的SR场景文本图像存在一定的问题，则会对识别任务造成很大的麻烦；同时，STISR任务模型的监督信号含有一定的识别损失而不是专门的超分辨率损失，因此并不会生成比较完整的SR场景文本图像。而本文实现的模型是如@fig1-1b 所示的并行架构，该架构首先使用统一的编码器将输入的图片映射到一个统一的特征空间，再通过不同任务的解码器输出相应的预测目标。该架构的优势在于多种任务使用之间的耦合度低，识别效果受到SR质量影响的程度较低，同时SR的质量也不受识别损失的干扰。

#figure(
  grid(
    columns: 2,
    gutter: 5%,
    [#subfigure(
      image("fig/fig1-1a.svg"),
      caption: [STISR和STR模型的串行架构]
    )<fig1-1a>],
    [#subfigure(
      image("fig/fig1-1b.svg"),
      caption: [STISR和STR模型的并行架构]
    )<fig1-1b>],
  ),
  caption: [两种STISR和STR结合的架构]
)<fig1-1>

== 本文研究内容

本文主要使用如@fig1-1b 所示的多任务模型架构，分别设计了特征编码器、特征增强模块、识别解码器和超分辨率解码器四个部分，并使用了两阶段训练和多任务损失函数进行模型监督训练，最终实现了从LR场景文本图像到文本内容和SR图像的端到端任务。本文的主要贡献如下：

+ 提出了一般性的LR场景文本图像识别和超分辨率的模型架构；
+ 本文的模型使用了基于隐式扩散模型的特征增强模块，并在推理时对扩散模型进行有效的控制，实现在特征层面上增强LR图像；
+ 模型采用两阶段的训练模型，在第一阶段进行预训练，目的是让模型保存高分辨率特征信息，在第二阶段进行特征增强，目的是提升LR图像特征信息，提升多任务模型的泛化能力；
+ 本文使用基于不确定性权重的损失函数进行多任务模型训练，实现STR任务和STISR任务之间的平衡；

== 论文各章节安排

本文主要讨论STR任务和STISR任务的多任务模型和特征增强方法，以及本文所实现的模型在数据集上的性能。
第二章介绍了目前现有的场景文本识别模型和场景文本图像超分辨率模型以及用于特征增强的扩散模型。
第三章将会着重介绍本文提出的模型架构、模型设计思路的实现方式，并会从各个模块出发，分别介绍该模型实现特征增强的方式和多任务解码方式。
第四章主要评估第三章中模型的性能，并展示了多种对比结果。
最后，第五章将会对本文提出的方法进行讨论和总结。

= 相关工作

本章将分别介绍场景文本识别（STR）模型和场景文本图像超分辨率（STISR）模型，并讨论其与本文模型的关系。除此之外，本章还将介绍本工作将要使用的TextZoom数据集，以便后文能顺利地引入本文模型。

== 场景文本识别

// * NOTES: cnn/rnn-based, attention-based, large model-based
// 1. cnn/rnn: crnn, aster,
// 2. attention: most popular, parseq, svtr, CCD, , SIGA, DPAN, MGP
// 3. large model: ABINet, CLIP4STR, DTrOCR, MATRN
// ! TODO: add formulas

#figure(
  image("fig/recognizer.svg", width: 75%),
  // placement: auto,
  caption: [STR识别器基本架构。其中虚线表示可选项]
)<fig2-rec>

与传统的光学字符识别（OCR）不同，场景文本识别（STR）主要针对场景中的字符进行识别，OCR面向的字符常常比较规整，而STR的识别对象经常由于拍摄问题，出现不同程度的透视或光影效果，有的甚至因为聚焦问题而出现低分辨率（LR）图像。对此问题，目前有很多研究人员提出了许多有效的方法，对高分辨率的场景文本进行识别，大部分模型的基本架构如@fig2-rec 所示，它们通常包含图像编码器用于将图像特征嵌入模型，再通过一系列的图像特征学习实现图像特征和文本特征的映射，最后使用激活函数将特征映射为标签，再经过损失函数将损失传播到模型参数，实现模型的训练。目前有研究人员在编码器和特征学习时使用卷积、循环神经网络以及注意力机制以便提取出更好的特征，除此之外，还有研究人员使用预训练的大语言模型对输出结果进行优化。
基于相关工作中使用的不同特征提取方式，本节从基于卷积和循环网络、基于注意力机制和基于大语言模型三个角度介绍目前常用的STR模型，以及他们在基准数据集上的性能。

*卷积和循环网络*。卷积和循环网络在计算时拥有绝佳的性能，用该模块设计的网络常常具有训练和推理速度快、模型体积小等特点，因此仍有大量研究人员使用其作为图片特征提取和文本信息处理的主要模块。
例如SVTR~@svtr 模型使用不同步长的卷积运算模拟ViT~@vit 将图片分割为多个小块（patch）的操作，再对其进行后续处理，实验证明，这种图片处理方式能较为高效地将图片嵌入模型。CRNN~@crnn 模型则使用多个卷积层深度提取图像特征，并使用双向的LSTM~@lstm 作为图片和文本之间的映射模块，最终使用CTC损失~@ctc 监督模型训练，其识别结果在当时取得了较高性能。而对于含有透视、扭曲结构等场景文本中，ASTER~@aster 模型在其主干网络之前添加了矫正模块，将弯曲文本矫正为水平排布的文本，并最终使用基于循环网络LSTM~@lstm 和GRU~@gru 的识别模块，最终在弯曲文本的识别中取得非常好的性能。
目前，大部分的模型都使用卷积提取图片特征，这种提取方式的优势在于能够尽量保存原有的图像特征，对于文本识别有很大帮助。本文实现的模型同样使用卷积作为编码器的主干。

*注意力网络*。注意力机制在NLP领域中得到了广泛应用，使用注意力机制能大幅提升模型对较长序列处理能力，不会出现类似LSTM等RNN模型在使用较长序列进行训练时出现的梯度消失或梯度爆炸的现象。对于STR任务，存在两种使用注意力机制的情况，一是针对文本进行注意力学习，再与图像特征进行交叉注意力学习，二是先将图像的特征提取后嵌入模型，再直接对图像进行注意力学习，最后用损失约束输出结果。
例如PARSeq~@parseq 使用了多种注意力机制进行训练。该模型的创新点在于使用排列数作为注意力机制的掩码，在文本特征层面使用了自注意力机制，并与图像特征共同使用交叉注意力，学习文本特征和图像特征中的映射关系，在当时的数据集上取得了较高性能。由于PARSeq模型在多个合成数据集上进行了大规模预训练，因此最终得到的网络具有很强的先验知识，可以根据预训练网络进行微调。
前文提到SVTR~@svtr 模型利用带有不同步长的卷积运算模拟ViT将图像嵌入网络，同时，该模型还使用多个注意力机制用于提取局部和全局特征，该注意力机制使用不同大小的掩码块引导模型对图像注意力的学习。
除此之外，MGP-STR~@mgp-str 以ViT为基础架构，将输入的文本图像分割为互不重叠小块。相较于ViT将不同的图像小块（即每个块的特征）聚合在一起作为整个图像的特征表示，在图像文本识别钟，MGP-STR使用了一种基于注意力机制模块对识别器进行多重约束，用于提取更加注重于文本的特征，将图像的小块有意义地聚合在一起，使得该模型可以预测字母、词组和字母个数等不同粒度的文本信息。
DPAN~@dpan 模型在架构上进行了研究，并基于并行解耦的编码器-解码器架构（Parallel-Decoupled Encoder Decoder, PDED），改进了其注意力的查询输入矩阵（query），从而弥补了查询矩阵和键值矩阵（key）之间的图像信息，提高了模型的鲁棒性。
尽管注意力机制的特征表示能力比卷积和循环模块强，但注意力机制可以让模型在全局特征的角度上进行计算，适合长度适中的特征序列，但其计算复杂度随着序列长度的增加而成平方及增长，不仅如此，其模型参数规模也远大于卷积和循环模块。

*大语言模型辅助网络*。在大语言模型（Large-Language Model, LLM）兴起后，很多与文本相关的任务都可以借助大语言模型实现。由于大语言模型使用大量的数据集进行训练，而模型也拥有巨大的参数量，因此训练出来的大语言模型有着很强的特征表示能力。早期的大语言模型如BERT~@bert 基于Transformer~@transformer 编码器设计了双向编码的无监督训练，其参数量达到了1.1亿（110 million~@bert ）。后续的GPT系列~@gpt 模型则使用自监督的方式进行训练，其中GPT-3~@gpt3 的参数量达到了1.75千亿（175 billion~@gpt3）。在研究图像和文本之间的关系中，CLIP~@clip 多模态模型使用对比学习，同时得到了文本和图像的特征表示编码器。大语言模型中拥有的大量参数使其具有很强的特征的表示能力，因此目前有研究人员直接使用大语言模型的编码器对下游任务中的数据集进行编码表示，并设计模型直接学习编码后的特征。
使用预训练的大模型可以在文本识别任务中取得较好性能。例如在CLIP4STR~@clip4str 工作中借助了视觉语言模型（Vision-Language Model, VLM），认为当前大部分识别器是基于单模态进行训练的，而VLM给予了文本和对应图像的信息，该模型通过视觉解码器和混合模态解码器的预测结果优化，在11个基准测试集中达到了较高的识别准确率。
与此同时，由于大语言模型含有丰富的上下文语义信息，因此有研究人员使用大模型进行文本预测结果的优化。例如ABINet~@abinet 设计了两个分支处理场景文本识别，其一是以残差网络和Transformer为基础架构，并且带有位置注意力的视觉分支，其二则是以CLIP为基础的，用于优化预测文本的语言模型分支。实验发现使用语言模型进行结果的迭代优化可以在基准数据集上达到较高识别准确率。
正如前文所述，使用预训练的大语言模型作为特征编码器或预测优化器，可以达到较高的识别准确率，同时由于大语言模型的强语义性，识别的结果绝大多数依然带有语义性，即识别结果是正确的单词。但在场景文本中，存在许多无上下文语义的文本，因此在使用大语言模型时需要权衡语义信息和图像信息之间的分配。

== 场景文本图像超分辨率

// * NOTES: interpolation based, attention based, diffusion based
// 0. introduce pixel shuffle module
// 1.interpolation based: bilinear and bicubic
// 2.attention based: TATT, DPMN,
// 3.diffusion based: SR3

#figure(
  image("fig/srer.svg", width: 75%),
  placement: auto,
  caption: [STISR超分器基本架构。其中虚线表示可选项]
)<fig2-sr>

场景文本图像超分辨率（STISR）的目的是提高低分辨率文本图像的质量，STISR能大大提升前文提到的STR任务对低分辨率文本的识别率，因此对该领域的研究对自动驾驶等下游任务的发展有着重要意义。与单张图像的超分辨率略有不同，STISR需要弥补图像特征和文本语义信息之间的模态差异，STISR的工作包含了文本的语义信息，在增强图像分辨率的同时不能破坏文本结构，因此STISR的约束条件比普通单张图像超分辨率更强。如@fig2-sr 所示，目前常用的模型框架通常包含一系列的编码器和特征学习模块，末尾通常使用像素重排模块~@pixelshuffle 提高图像分辨率。目前，已经有很多研究人员在该领域做出了巨大贡献，例如有研究人员将文本先验知识引入图像的特征学习模块中，用先验特征引导超分辨率，还有研究人员在SR图像的基础上进一步做结果的优化。本节先介绍传统插值的图像超分辨率方法，接着以该方法为基础，引出当前该领域的相关工作。

*插值*。插值主要目的是根据给定数据点，估计给定点附近的值。用于图像超分辨率的插值方式包括最近邻插值、双线性插值和双三次插值等方式。
如@fig2-2-1 假设原始图像大小为 $(h, w)$，以将图片分辨率放大到原来的*两倍*为例，即超分辨后的图像大小为 $(2h,2w)$，分别讨论三种超分辨率方式的效果。对于一个像素点，最近邻插值忽略其周围像素值，直接在周围扩展该像素的值，其效果如@fig2-2-1 （c）所示，可以发现其锯齿化比较严重。而双线性插值会考虑到以该像素点为顶点的周围像素值，假设四个顶点分别为 $A(x_1,y_1),B(x_1,y_2),C(x_2,y_2),D(x_2,y_1)$，这些点所代表的像素值分别由 $f(A),f(B),f(C),f(D)$ 给出，则该四个顶点间任意一个点的像素值 $P(x,y)$ 由公式@eq2-2-1 @eq2-2-2 和@eq2-2-3 给出。其插值效果如@fig2-2-1 （d）所示。相较于最近邻插值，双线性插值在一定程度上减轻了锯齿化。而双三次插值插值一次使用的像素点个数是双线性插值的 $4$ 倍，即使用 $16$ 个像素值估计一个像素值，类似于双线性插值其基本思想是使用三次多项式函数拟合给定的四个像素值，并通过一阶导数和二阶混合偏导保证连续性，从而得到 $16$ 个方程组成的方程组，解出 $16$ 个顶点对应的权重。双三次插值的效果如@fig2-2-1 （e）所示，可以看出其效果比双线性插值多了更多的图像细节。

#figure(
  image("fig/fig2-2-1.svg", width: 75%),
  placement: auto,
  caption: [（a）表示原始高分辨率图像 $(2h,2w)$，（b）表示压缩后大小为 $(h,w)$ 的图像等比放大后的低分辨率图像 $(2h,2w)$，（c）表示使用最近邻插值方式的超分图像 $(2h,2w)$，（d）表示使用双二次插值的超分图像 $(2h,2w)$，（e）表示使用双三次插值的超分图像 $(2h,2w)$]
)<fig2-2-1>

$
f(P_y_1)=(x_2-x)/(x_2-x_1) f(A) + (x-x_1)/(x_2-x_1) f(B)
$ <eq2-2-1>

$
f(P_y_2)=(x_2-x)/(x_2-x_1) f(D) + (x-x_1)/(x_2-x_1) f(C)
$ <eq2-2-2>

$
f(P)=(x_2-x)/(x_2-x_1) f(P_y_1) + (x-x_1)/(x_2-x_1) f(P_y_2)
$ <eq2-2-3>

使用插值的方式进行图像超分辨率，是在图像的宽和高两个维度上进行像素扩展，在使用深度学习模型进行超分辨率时，需要先将LR图像插值到SR图像，再使用卷积、注意力机制等模块增强其分辨率。而近年来提出的像素重排（PixelShuffle）模块~@pixelshuffle 可以直接在LR图像上提取特征，它的主要思想是将通道维度特征重组到宽和高维度上，该方式既能提高图像分辨率，又能在通道维度上提取特征，是当前网络增强图像分辨率的主要方式。如@fig2-2-2 所示，像素重排主要基于网络提取的特征图，特征图的通道维度上是具有超分辨像素信息的特征，通过重排后，通道维度的像素信息补全到宽和高两个维度，从而增强图像分辨率。与传统的插值方式相比，这种在通道维度上提取特征的方式可以减少网络传播中特征图的大小，同时也更适合具有卷积模块的网络在通道层面上提取特征的方式。基于该模块，有很多研究人员提出了新的超分辨率模型，并在基准数据集上取得了较好的结果。

#figure(
  image("fig/fig2-2-2.svg", width: 75%),
  placement: auto,
  caption: [像素重排（PixelShuffle）模块的主要操作原理。原特征图的通道维度被重排至宽和高两个维度，从而提高图像的分辨率]
)<fig2-2-2>

*模型*。使用卷积和循环网络搭建模型的代表性工作是TSRN~@tsrn，同时该模型的作者也是STISR任务的提出者，并创建了新的数据集用于训练和测试。该模型的主要思路是残差学习，模型使用了重复的超分子模块SRB~@tsrn 对超分图像的残差进行学习后，将残差与浅层特征进行加和，最后使用像素重排输出较高分辨率的图像。在整个网络的特征传播中，特征图的大小保持一致且与LR图像大小相同，模型仅在通道维度上进行特征学习，与前文介绍的像素重排相互印证。
TSRN是STISR领域的开山之作，因而有研究人员延续其特点，提出了新的架构。例如TATT模型~@tatt 在此基础上增加了注意力机制，目的是混合先验文本信息和图像特征，在一定程度上用文本的先验特征引导超分辨率的进行。TATT模型在LR图像输入时，通过预训练识别器对LR图像先进行一次识别得到预测的文本标签，同时用多个卷积模块提取LR图像的图像特征，再将二者输入到带有交叉注意力的特征融合模块，最后通过TSRN的主干网络输出超分辨率图像。与此思想类似的工作还包括TPGSR~@tpgsr ，该模型同样使用了文本先验模块对超分辨率模块进行引导，并同时使用由高分辨率图像产生的文本先验约束LR图像的文本先验，以便生成更可靠的文本先验。由于大部分超分网络具有一定的缺陷，有研究人员基于这些超分网络，设计了即插即用的超分辨率模块用于提高预训练超分模型的性能。DPMN~@dpmn 则从优化的角度出发，在原有的超分辨率模型的基础上添加即插即用的性能提升模块，将现有的超分辨率模型输出的SR图像作为DPMN模块的输入，输入PGRM（Prior-Guided Refinement Module）后使用预训练的ViT提取先验信息，并对其做像素重排，将多个PGRM的输出再进行组合，最终得到分辨率更高的SR图像。PCAN~@pcan 则训练出用于提取超分语义信息的注意力模块，并提出了新的损失函数用于模型的监督训练。STT~@stt 模型则使用注意力机制关注单个文本的字形的细节特征，并在特征图层面上用损失函数进行监督。而C3-STISR~@c3 则从不同特征理解的角度出发，利用注意力机制权衡识别、图像和语义三种理解，在基准数据集上超过同期模型。而随着DDPM~@ddpm 等扩散模型（Diffusion Model）的提出，研究人员发现该模型在图像恢复上面有一定的效果，因此部分研究人员将DDPM及其衍生模型引入STISR领域，扩充了文本图像的超分辨率模型类型。DDPM在原图上进行多步加噪和多步去噪过程，用模型预测加的噪声，最终推理时用纯噪声逐步减去预测的噪声，实现图像的生成。由于原始的DDPM生成效果带有随机性，因此SR3~@sr3 基于隐式扩散模型（Stable Diffusion）将LR图像作为条件概率中的条件，引导生成图像的效果，最终在单张图像超分上取得了很好的效果。在STISR任务上，TextDiff~@textdiff 首次使用扩散模型进行残差学习，在基准数据集上取得良好效果。

== 数据集

#figure(
  placement: auto,
  grid(
    columns: 3,
    gutter: 1%,
    [#subfigure(
      image("fig/stat_ds.svg"),
      caption: [各个数据集样本量]
    )<fig2-2-3a>],
    [#subfigure(
      image("fig/stat_char.svg"),
      caption: [标签文本类型]
    )<fig2-2-3b>],
    [#subfigure(
      image("fig/stat_length.svg"),
      caption: [标签文本长度]
    )<fig2-2-3c>],
  ),
  caption: [TextZoom数据集标签特征统计]
)<fig2-2-3>

#figure(
  placement: auto,
  grid(
    columns: 2,
    gutter: 15%,
    [#subfigure(
      image("fig/train_stat.svg"),
      caption: [训练集PSNR与SSIM分布图]
    )<fig2-2-4a>],
    [#subfigure(
      image("fig/test_stat.svg"),
      caption: [测试集PSNR与SSIM分布图]
    )<fig2-2-4b>],
  ),
  caption: [TextZoom数据集图像特征统计。PSNR和SSIM越高表示LR图像质量越高]
)<fig2-2-4>

对于单独的STR任务而言，目前绝大部分识别器需要在大规模的人造数据集和真实数据集上进行训练，得到的模型通常包含了与文本相关的语义信息，可以在诸多下游任务的数据集上进行微调。用于大规模训练的人造数据集有包含了900K样本量的MJSynth~@mjsynth 和包含了800K样本量的SynthText~@synthtext。当前已经有很多STR工作在这些数据集上进行训练，用于后续模型的微调。而用于STISR和STR双任务的数据集目前常用的是TextZoom~@tsrn ，该数据集由两个单张图片超分辨率的基准数据集RealSR~@realsr 和SR-RAW~@sr-raw 筛选和截取而成，其中标签的分布如@fig2-2-3 所示，图像的分布如@fig2-2-4 所示。TextZoom分为训练集（包含验证集）、简单难度的测试集、中等难度的测试集和困难难度的测试集，各个数据集的样本量如@fig2-2-3a 所示。如@fig2-2-3b 所示，在文本字符角度，数据集的标签文本类型包含了数字和大小写英文字母，每个图像的标签不仅包含数字，也有可能同时包含大小写的英文字母。如@fig2-2-3c 所示，数据集大部分的标签长度在$10$以内，且各个长度的分布比较均匀。
在图像方面，图像的质量由PSNR（Peak Signal Noise Ratio）和SSIM（Structural Similarity Index Measure）反映，指标的值越高，则两张图像越接近。在@fig2-2-4 中，本文统计了LR图像相对于HR图像的PSNR和SSIM，可以发现训练集和测试集在分布上有一定的相似性，因此使用TextZoom训练集作为模型的训练数据。而由于测试集图像质量比较分散，因此使用不同难度的LR图像作为模型性能的评估数据集也是合理的。

// 同前文所述，使用在大规模人造数据集和真实数据集上预训练的识别器进行微调可以给模型带来一定的先验语义特征，TextZoom数据集和MJSynth数据集在标签和图像分布上存在一定差异，适合用作微调数据集。因此本文识别器采用一种在MJSynth数据集上预训练后的识别器作为主干，并研究了相应的微调方法，最终取得较好的识别性能。

= 模型原理与设计

本章首先介绍模型的总体架构及其形式化描述，再分别详细介绍模型各个模块的设计和实现效果，最后给出模型训练所使用的损失函数以及训练和推理算法。

== 模型总体架构

#figure(
  image("fig/arch.svg", width: 100%),
  // placement: auto,
  caption: [模型整体架构。模型编码器和解码器（灰色虚线框内部）在各个阶段都处于激活状态，特征增强部分（红色虚线框内部）只在特征增强阶段被激活和训练]
)<fig3-arch>

不失一般性，模型的输入和输出都是以一个批量大小进行，超分辨率的倍数为$2$，假设输入的LR低分辨率图像用 $I_l$ 表示，对应的HR高分辨率图像用 $I_h$ 表示，而超分辨率模型的输出SR为 $I_s$，图像对应的真实标签和识别器预测标签在嵌入模型后，分别为 $S_t, S_p$。图像和嵌入标签维度分别满足：
$I_l in bb(R)^(B times C times H times W), I_h in bb(R)^(B times C times 2H times 2W), I_s in bb(R)^(B times C times 2H times 2W), S_t in bb(R)^(B times L_t), S_p in bb(R)^(B times L_p)$
其中 $B$ 表示输入和输出数据的批量大小，$C$ 表示输入和输出图像的通道数，通常为 $3$，而 $H,W$ 分别表示LR图像的高和宽，由于放大倍数是 $2$，因此SR图像和HR图像的高和宽分别是 $2H,2W$，对于标签，$L_t,L_p$ 分别表示了真实标签的长度和预测标签的长度。

该模型的目标是通过训练集 $cal(S)={(L_t,I_h,I_l)_i}_(i=0)^N$ 训练模型 $cal(F)_theta$，使其能够以 $I_l$ 为输入，预测对应的标签 $L_p$ 和重建SR图像 $I_s$，即

$
cal(F)_theta (I_l)=(L_p,I_s)
$

从目前研究中发现，现有的模型多是基于串行结构搭建，即先对LR图像进行超分辨率，提高文本图像的分辨率，再用识别器进行文本预测。类似这一类的串行结构理解简单、训练更为方便，同时有很多研究人员达到了部分预期效果。然而这种框架经常会带来一些问题：
1. 首先，由于识别结果的准确率大多基于SR图像的质量，因此如果超分辨率模块的输出结果并不可靠，从而生成错误的超分辨率图像，就极有可能导致后续识别器预测出错误的文本。
2. 其次，两个任务是串行关系，因此如果不使用梯度截止等方法，后一个模块的损失在进行传播时会影响到前一个模块参数的更新，从而导致超分辨率模块被识别损失监督，原有的超分损失受到影响，从而生成的SR图像并不完全有着较高的保真度。

为解决以上问题，本文使用了一种新型的多任务并行架构，如@fig3-arch 所示，该模型的识别模块和超分模块处于并行关系，二者共享图像的编码特征，但解码特征相互独立。在使用多个损失函数进行监督训练时，两个解码器会更加注重于更新参数使得与其对应的损失降低，而不会受到另一个损失的影响。其次由于识别器从特征层抽取特征，受到超分辨率效果的影响较小，两个任务的耦合度大大降低。为了 进一步提升图像特征的质量，该模型还设计了特征增强模块，提高LR图像的特征质量。设编码器为 $cal(E)_theta$，识别器为 $cal(D)^r_theta$， 超分器为 $cal(D)^s_theta$，特征增强模块为 $cal(A)_theta$，编码器输出特征为 $F_e in bb(R)^(B times C_e times H_e times W_e)$，特征增强模块输出为 $F_a in cal(R)^(B times C_a times H_a times W_a)$ （其中 $C_x,H_x,W_x$ 分别表示中间特征的通道数、高度和宽度）该架构可以由@eq3-1 表示：在推理阶段，模型的输入为LR低分辨率图像，先经过编码器进行初步的特征提取，再由特征增强模块提取LR特征潜在的超分辨率图像特征和文本特征，最后将含有超分辨率和文本识别混合特征的张量图输入不同任务的解码器，从而输出识别文本和SR超分辨率图像，实现模型对LR图像的识别和超分辨率。

$
cal(F)_theta (I_l) &= (cal(D)^r_theta (F_a),cal(D)^s_theta (F_a)),\
F_a &= cal(A)_theta (F_e),\
F_e &= cal(E)_theta (I_l)
$<eq3-1>

整个模型的训练方式可以分为两个阶段：预训练阶段和特征增强阶段。

*预训练阶段*。该阶段特征增强模块不接入模型。模型主要使用HR图像训练识别器分支和超分辨率分支。该阶段目的是将HR图像的编码特征保留在编码器中，让编码器学习到HR图像的特征解码方式，以便后续能以类似的方式提取LR图像的特征。其次是预训练超分器和识别器，提高超分器的重建效果和识别器对编码特征的识别性能。

*特征增强阶段*。该阶段主要训练特征增强模块，需要将其接入模型。特征增强思路是使用LR图像和HR图像共同进行训练，使得该模块能输出与HR编码后特征相近的超分辨率特征。同时，该阶段需要微调解码器的两个分支，以便弥补SR特征和HR特征之间的分布差异。


== 编码器模块设计

编码器主要负责对齐图像和混合特征。

//TODO: 补充图片说明，附录补充公式推导
*图像中心对齐*。由于数据集通过失焦采样真实的场景文本，LR图像中心和HR图像中心会出现一定的偏移，因此模型在编码器部分设计了空间变换网络（Spatial Transformer Network, STN）~@stn 用于对图像做空间尺度的变换，将LR图像与HR图像中心对齐。同时编码器还使用薄板样条插值（Thin Plate Spline, TPS）~@tps 扩大STN变换的灵活性。STN模块实现对图像变换的预测，增强卷积对图像剪切、旋转和仿射等变换的鲁棒性，TPS模块使用插值的方式，通过关注图像控制点的变换，实现估计图像除控制点以外其余点的位置。该部分的基本流程为：先通过STN预测输入图像变换后控制点的位置，再将控制点和图像输入TPS实现图像的变换。
//设有空间形变变换 $Phi$，将LR图像映射到HR图像，由TPS~@tps 可以解出形变函数为 

// $
// cal(L)=cal(L)_Phi + lambda cal(L)_d\
// cal(L)_Phi = sum_i^N ||Phi(p_i) - p_i||_2^2\
// cal(L)_d = integral integral_(bb(R)^2) ((partial^2 Phi) / (partial x^2))^2 + ((partial^2 Phi) / (partial x partial y))^2 + ((partial^2 Phi) / (partial y^2))^2 d x d y
// $

// 其中 $cal(L)_Phi$ 表示将LR图像的一点映射到HR图像后的距离大小，$cal(L)_d$ 表示扭曲的能量函数~@tps，$lambda$ 用于控制扭曲程度，$lambda$ 越大，表示LR到HR的形变的扭曲程度越大。由最小化 $cal(L)$ 可以解出形变函数的闭式解

// $
// Phi(p) = a dot p + b + sum_i^N omega_i U(||p-p_i||)
// $

// 其中 $U(||p-p_i||)$ 为径向基函数 $U(r)=r^2 log r^2, r= sqrt(x^2+y^2)$，前者 $y=a dot p + b$ 是三维空间中的平面

*混合特征*。编码器的定位特殊，作为不同任务的前置特征提取模块之一，编码器的设计和训练需要多次实验测试。由第二章@fig2-rec 可知，通常的识别器包含图像特征编码器和后续的序列特征学习模块，而由第二章@fig2-sr 可知，通常的超分器同样包含图像特征编码器和后续的超分图像特征学习模块。因此本文设想通过构造一种以卷积为主的特征提取编码器，同时实现文本特征的提取和超分特征的提取。预训练阶段，编码器输入的是HR高分辨率图像，超分模块的重建任务较为简单，该阶段需要注重文本特征的提取；特征增强阶段，需要在编码器后接入特征增强模块，此时模型的输入是LR低分辨率图像，此时预训练完成的编码器会先对LR低分辨率图像进行特征提取，接着将LR低分辨率特征进行增强，输出适应预训练解码器的超分特征，该超分特征含有文本信息。因而在编码器设计部分，本文主要利用编码器实现图像文本信息的浅层提取，而超分变率特征主要由后续的特征增强模块实现。

// TODO: 补充编码器和SRB、GRU的图，引用
该模块的设计如\@所示，输入的图像先统一调整大小，使其与推理时输入的LR图像大小一致，具体而言，如果输入的是大小为 $(2h,2w)$ 的HR图像，则使用最近邻插值调整大小到 $(h,w)$；而如果输入是LR图像，则不进行大小调整。调整大小后的图像先经过卷积网络映射到特征层，再使用SRB~@tsrn 整合图像特征和文本特征。其中SRB的设计如\@所示，进入BiGRU模块前的特征图进行维度调整：先切割为纵向排列的条状特征（设该操作为$f$），再将这些条状特征依次连接（设该操作为$g$），即

$
g(f(F)):bb(R)^(B times C times H times W) -> bb(R)^((B H) times W times C)\
f(F):bb(R)^(B times C times H times W) -> bb(R)^(B times H times W times C)\
g(F):bb(R)^(B times H times W times C) -> bb(R)^((B H) times W times C)
$

维度调整能够实现图像特征和文本特征的融合，相邻的条状特征更有可能对应于一个文本。处理后的特征图送入BiGRU进行特征提取，输出的特征映射回原维度。最后将输出特征与卷积特征通过残差进行连接，输入到下一个模块。为了提高编码器特征提取能力，编码器内部堆叠了两个SRB模块。预训练阶段，编码器输出的特征包括两部分，第一部分的特征会经过像素重排输入识别器分支，第二部分不经过像素重排，送入超分辨率分支继续处理超分辨率特征。特征增强阶段和推理阶段，编码器输出的特征是LR特征，不会经过像素重排，直接输入特征增强模块做进一步的特征提升。编码器的训练和解码器的训练方式一致，多个损失函数使用不确定性加权进行权衡，这一点将在本章最后一节进行讨论。

== 特征增强模块设计

特征增强模块主要负责增强的编码器输出的LR特征。

#figure(
  image("fig/gru.svg", width: 80%),
  caption: [SRB基本模块和特征增强模块]
)<fig3-srb>

//TODO: 补充图
与编码器类似，特征增强模块的主干网络使用SRB进行堆叠，但在整个模块中使用了更长的残差连接，并且该模块训练方式与编码器有所不同。如\@所示，特征图输入后先保留一份备份用于残差连接，再经过堆叠的$N$个SRB模块提取特征，接着通过卷积层融合图像和文本特征，最后使用残差连接将输出的特征图与进入网络的特征图相加，同3.2节所述，模块分别输出大小不同的特征图分别输入识别器和超分器。与编码器不同的是，该部分在训练时会使用额外的特征损失进行监督，从而使该模块的特征偏移和整个模型相似。为了使用合适个数的SRB模块，本文在第四章对增强模块的SRB个数做了消融实验。

== 双任务解码器设计

双任务解码器分为识别器和超分器。识别器负责将输入的特征图解码为文本。超分器负责将特征图重建为SR超分辨率图像。
// //TODO: 图有问题，少了❄
// #figure(
//   image("fig/crnn.svg", width: 80%),
//   placement: auto,
//   caption: [识别器模块。其中实线部分的模块和路线为预训练的CRNN主干，虚线部分的模块和路线为在微调时加入的额外的模块和路线]
// )<fig3-crnn>

// 目前绝大多数识别器需要在大规模人造数据集或真实数据集上进行训练，其中常用的人造数据集有MJSynth~@mjsynth 和SynthText~@synthtext。在大规模数据集上进行预训练的STR模型通常有较好的语义特征，泛化能力较强，而TextZoom~@tsrn 数据集规模远小于以上两种，且数据分布存在一定差异，适合作为微调数据集。经过调研，本文识别器选取CRNN~@crnn 作为主干网络，并用该网络在TextZoom数据集上做微调训练。如@fig3-crnn 所示，实线部分为CRNN主干网络，分为CNN模块、MLP、LSTM模块和最后的分类头。虚线部分为本文对其的微调策略。该模型的CNN部分在网络前部，主要提取图像的局部特征，预训练好的模型该部分的特征提取能力较强；而LSTM部分在网络较深处，接近分类头，特征更加完整。在冻结原有模型的基础上，本文对该网络的微调策略为在LSTM模块上增加旁路LSTM模块（bypass LSTM），重新学习部分的完整特征，该方法可以较大程度上防止网络出现遗忘等现象。除此之外，本文扩充了原有的分类头，在其基础上增加了两层MLP，将原有的LSTM主干网络和微调的LSTM旁路模块特征进行融合，再进行一次分类。

// == 超分辨率模块设计

// 超分辨率模块主要将输入的特征解码为SR超分图像。

// 超分辨率模块在预训练阶段学习HR图像的重建过程，本文在预训练时用该模块拟合HR图像进一步插值后的SR图像，使得该模块在实际超分增强后的LR特征具有更强的SR能力。相比于特征增强模块对特征图进行特征空间内的映射，该部分将特征图映射到图像空间。
//TODO: 图
*识别器*。常见的工作中可以使用任何预训练识别器作为主干，只需要微调其分类头即可在相应数据集上实现较高的准确率。预训练模型中的编码器部分是用于提取图像特征的关键部分，该部分能提取多量、浅层的细节特征，而解码器部分用于处理这些浅层特征，并逐步整合为完整的深层特征，最后分类头和激活函数将深层特征映射为几率（logits）。本文的识别器受到CRNN~@crnn 的启发，在CNN和LSTM之间加入了MLP和残差连接，同时增强了模型分类头，如\@所示。由于特征图直接由本文模型的编码器或特征增强模块得到，因此本文重新训练了识别器。在消融实验中，本文将讨论微调预训练识别器和重新训练识别器之间的差别。

*超分器*。如\@所示，超分器的实现较为简单，仅使用一个SRB模块和残差连接进行特征提取，再经过像素重排和卷积层即可输出SR超分辨率图像。

== 损失函数

模型使用多个损失函数进行监督训练，并学习不确定性权重（Uncertainty Weight Loss）~@uncertainty-loss 来权衡不同任务之间的比重。

*识别损失*。识别损失采用CTC损失函数~@ctc，该损失常用于文本序列预测的监督任务。在序列模型中，采用交叉熵损失或均方误差损失时，经常需要将输出的预测值与真实值标签对齐，这种对齐方式工作量大；除此之外，预测结果中可能存在空白字符和连续字符，空白字符可能由图像中两个文本间的空白区域预测得到，而连续字符则可能是图像中同一字符被多次识别，也有可能是文本中原本就存在连续字符；因此解码器的工作量较大。CTC损失通过在标签中添加空白字符分隔每个字符，在解码时先合并相同字符，再删去空白符得到预测标签，不需要事先对齐标签长度，由于两个连续字符之间必然会存在空白字符，因而不会造成预测歧义。

= 实验结果与分析

== 模型总体性能

== 与目前工作的对比

= 总结与展望

#seu_bibliography("../ref.bib")

#appendix[
  = 模型推理结果
]

#acknowledgment[]