#import "includes/template.typ": bachelor_conf, seu_bibliography
#import "includes/template.typ": acknowledgment, appendix
#import "includes/utils.typ": set_doc_footnote, subfigure
#import "@preview/funarray:0.3.0":cycle
#import "@preview/tablex:0.0.8": tablex,vlinex,hlinex,colspanx,rowspanx
#show :doc => set_doc_footnote(doc)


#let info = (
  title: [面向低分辨率场景文本图像的增强与识别技术研究],
  student_id: [61520324],
  name: [许睿],
  college: [吴健雄学院],
  major: [计算机科学与技术],
  supervisor: [薛晖],
  duration: "2023.12~2024.6",

  zh_abstract: [
    中文摘要
  ],

  zh_key_words: (
    "关键字1",
    "关键字2"
  ),

  en_abstract: [
    #lorem(250)
  ],

  en_key_words: (
    "Keywords1",
    "Keywords2"
  )
)


#show :doc => bachelor_conf(
  doc,
  ..info
)

#figure(
  tablex(
    auto-lines: false,
    columns: cycle((1fr, 2fr, 1fr), 3),
    stroke: 1pt,
    align: left+horizon,

    hlinex(),
    [术语], [英文], [中文],
    hlinex(),
    [STR], [Scene-Text Recognition], [场景文本识别],
    [STISR], [Scene-Text Image Super-Resolution], [场景文本图片超分],
    [NLP], [Natural Language Processing], [自然语言处理],
    [OCR], [Optical Character Recognition], [光学字符识别],
    [LR], [Low Resolution], [低分辨率],
    [HR], [High Resolution], [高分辨率],
    [SR], [Super-Resolution], [超分辨率],
    hlinex()
  ),
  kind: table,
  caption: [中英术语对照表]
)

= 绪论

== 课题背景和意义

从一般性的场景图片中识别文本信息不仅能帮助深度学习模型在训练时理解场景逻辑，还能在推理时给予使用者更多场景相关的信息，该类型的任务被称为场景文本识别（Scene-Text Recognition, STR）。
场景文本识别能在诸多实际应用中得到重要应用：例如在辅助驾驶中，汽车可以动态识别路上的交通符号，以便根据路况和交通规则做出安全的决策。
// TODO: 多个引用只显示首尾两个
近年来有许多研究人员躬身场景文本识别领域并取得了很多优秀成果。这些模型在目前广泛使用的基准数据集 ~@cute80~@icdar2013~@coco-text 中对于高分辨率（High-Resolution, HR）的图像取得了非常高的准确率。
现实中的STR任务比基准数据集的形式更为复杂，被识别的字形可能因设计要求等因素，单个字符可以有多种字体和不同程度的扭曲，因此也有很多研究人员提出可以使用矫正模块 @aster 对场景文本进行空间上的矫正，使其便于识别。因此给一张比较清晰的场景文本图像，当前有许多方式可以以较高置信度对其进行识别。

然而，在场景文本图像中的特征受到大量影响时，识别器的性能也大幅下降。例如，受到拍摄环境或拍摄器材的制约时，拍摄的场景文本图像可能带有一定的模糊，这对识别器的鲁棒性带来了巨大的挑战。
因此有研究人员 @tsrn 提出了更加一般的STR任务，即对于带有模糊等不良因素的低分辨率（Low-Resolution, LR）图像，可以先进行场景文本图像超分辨率（Scene-Text Image Super-Resolution, STISR）处理，得到较为清晰的超分辨率（Super-Resolution, SR）图像后，再进行常规的STR。该研究人员构建了专门的模糊场景文本数据集 @tsrn 用于评判模型在不良条件下的超分辨率和识别能力。与传统的图像超分辨率（Image Super-Resolution）任务不同，STISR任务更加注重于文本的恢复，由于不少文本包含一定的语义信息，这不仅给图像超分辨率模型带来了挑战，也使STISR模型得到更多新的思路。

对于LR场景文本的识别，当前广泛使用的模型架构是如@fig1-1a 所示的串行架构。这种架构虽然直观，但同时存在不少问题。串行架构中，两个任务的耦合程度太高，STR任务非常依赖于STISR任务的性能，如果STISR得到的SR场景文本图像存在一定的问题，则会对识别任务造成很大的麻烦；同时，STISR任务模型的监督信号含有一定的识别损失而不是专门的超分辨率损失，因此并不会生成比较完整的SR场景文本图像。而本文实现的模型是如@fig1-1b 所示的并行架构，该架构首先使用统一的编码器将输入的图片映射到一个统一的特征空间，再通过不同任务的解码器输出相应的预测目标。该架构的优势在于多种任务使用之间的耦合度低，识别效果受到SR质量影响的程度较低，同时SR的质量也不受识别损失的干扰。

#figure(
  grid(
    columns: 2,
    gutter: 5%,
    [#subfigure(
      image("fig/fig1-1a.svg"),
      caption: [STISR和STR模型的串行架构]
    )<fig1-1a>],
    [#subfigure(
      image("fig/fig1-1b.svg"),
      caption: [STISR和STR模型的并行架构]
    )<fig1-1b>],
  ),
  caption: [两种STISR和STR结合的架构]
)<fig1-1>

== 本文研究内容

本文主要使用如@fig1-1b 所示的多任务模型架构，分别设计了特征编码器、特征增强模块、识别解码器和超分辨率解码器四个部分，并使用了两阶段训练和多任务损失函数进行模型监督训练，最终实现了从LR场景文本图像到文本内容和SR图像的端到端任务。本文的主要贡献如下：

+ 提出了一般性的LR场景文本图像识别和超分辨率的模型架构；
+ 本文的模型使用了基于隐式扩散模型的特征增强模块，并在推理时对扩散模型进行有效的控制，实现在特征层面上增强LR图像；
+ 模型采用两阶段的训练模型，在第一阶段进行预训练，目的是让模型保存高分辨率特征信息，在第二阶段进行特征增强，目的是提升LR图像特征信息，提升多任务模型的泛化能力；
+ 本文使用基于不确定性权重的损失函数进行多任务模型训练，实现STR任务和STISR任务之间的平衡；

== 论文各章节安排

本文主要讨论STR任务和STISR任务的多任务模型和特征增强方法，以及本文所实现的模型在数据集上的性能。
第二章介绍了目前现有的场景文本识别模型和场景文本图像超分辨率模型以及用于特征增强的扩散模型。
第三章将会着重介绍本文提出的模型架构、模型设计思路的实现方式，并会从各个模块出发，分别介绍该模型实现特征增强的方式和多任务解码方式。
第四章主要评估第三章中模型的性能，并展示了多种对比结果。
最后，第五章将会对本文提出的方法进行讨论和总结。

= 相关工作

本章将分别介绍场景文本识别（STR）模型、场景文本图像超分辨率（STISR）模型和隐式扩散模型（Stable Diffusion Model），并讨论其与本文模型的关系。

== 场景文本识别

// TODO: cnn/rnn-based, attention-based, large model-based
// 1. cnn/rnn: crnn, aster,
// 2. attention: most popular, parseq, svtr, CCD, MATRN, SIGA, DPAN, MGP
// 3. large model: ABINet, CLIP4STR, DTrOCR, 
与传统的光学字符识别（OCR）不同，场景文本识别（STR）主要针对场景中的字符进行识别，OCR面向的字符常常比较规整，而STR的识别对象经常由于拍摄问题，出现不同程度的透视或光影效果，有的甚至因为聚焦问题而出现低分辨率（LR）图像。对此问题，目前有很多研究人员提出了许多有效的方法，对高分辨率的场景文本进行识别，有的模型还通过额外设计的模块增强图片中的文本特征。基于不同架构，本节从基于卷积和循环网络、基于注意力机制和基于大语言模型三个角度介绍目前常用的STR模型，以及他们在基准数据集上的性能。

*卷积和循环网络*。卷积和循环网络在计算时拥有绝佳的性能，用该模块设计的网络常常具有训练和推理速度快、模型体积小等特点，因此仍有大量研究人员使用其作为图片特征提取和文本信息处理的主要模块。
例如SVTR~@svtr 模型使用不同步长的卷积运算模拟ViT~@vit 将图片分割为多个小块（patch）的操作，再对其进行后续处理，实验证明，这种图片处理方式能较为高效地将图片嵌入模型。CRNN~@crnn 模型则使用多个卷积层深度提取图像特征，并使用双向的LSTM~@lstm 作为图片和文本之间的映射模块，最终使用CTC损失~@ctc 监督模型训练，其识别结果在当时取得了较高性能。而对于含有透视、扭曲结构等场景文本中，ASTER~@aster 模型在其主干网络之前添加了矫正模块，将弯曲文本矫正为水平排布的文本，并最终使用基于循环网络LSTM~@lstm 和GRU~@gru 的识别模块，最终在弯曲文本的识别中取得非常好的性能。
目前，大部分的模型都使用卷积提取图片特征，这种提取方式的优势在于能够尽量保存原有的图像特征，对于文本识别有很大帮助。本文实现的模型同样使用卷积作为编码器的主干。

*注意力网络*。注意力机制在NLP领域中得到了广泛应用，使用注意力机制能大幅提升模型对较长序列处理能力，不会出现类似LSTM等RNN模型在使用较长序列进行训练时出现的梯度消失现象。对于STR任务，存在两种使用注意力机制的情况，一是针对文本进行注意力学习，再与图像特征进行交叉注意力学习，二是先将图像的特征提取后嵌入模型，再直接对图像进行注意力学习，最后用损失约束输出结果。
例如PARSeq~@parseq 则使用了多种注意力机制进行训练。该模型的创新点在于使用排列数作为注意力机制的掩码，在文本特征层面使用了自注意力机制，并与图像特征共同使用交叉注意力，学习文本特征和图像特征中的映射关系。

== 场景文本图像超分辨率


== 扩散模型


== 多任务损失函数


== 数据集


= 模型原理与设计


== 模型总体架构


== 编码器模块设计


== 特征增强模块设计


== 识别器模块设计


== 超分辨模块设计


= 实验结果与分析

== 模型总体性能

== 与目前工作的对比

= 总结与展望

#seu_bibliography("../ref.bib")

#appendix[
  = 扩散模型推导
  = 模型推理结果
]

#acknowledgment[]