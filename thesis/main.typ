#import "includes/template.typ": bachelor_conf, seu_bibliography
#import "includes/template.typ": acknowledgment, appendix
#import "includes/utils.typ": set_doc_footnote, subfigure
#import "@preview/funarray:0.3.0":cycle
#import "@preview/tablex:0.0.8": tablex,vlinex,hlinex,colspanx,rowspanx
#show :doc => set_doc_footnote(doc)


#let info = (
  title: [面向低分辨率场景文本图像的增强与识别技术研究],
  student_id: [61520324],
  name: [许睿],
  college: [吴健雄学院],
  major: [计算机科学与技术],
  supervisor: [薛晖],
  duration: "2023.12~2024.6",

  zh_abstract: [
    中文摘要
  ],

  zh_key_words: (
    "关键字1",
    "关键字2"
  ),

  en_abstract: [
    #lorem(250)
  ],

  en_key_words: (
    "Keywords1",
    "Keywords2"
  )
)


#show :doc => bachelor_conf(
  doc,
  ..info
)

#figure(
  tablex(
    auto-lines: false,
    columns: cycle((1fr, 2fr, 1fr), 3),
    stroke: 1pt,
    align: left+horizon,

    hlinex(),
    [术语], [英文], [中文],
    hlinex(),
    [STR], [Scene-Text Recognition], [场景文本识别],
    [STISR], [Scene-Text Image Super-Resolution], [场景文本图片超分],
    [NLP], [Natural Language Processing], [自然语言处理],
    [OCR], [Optical Character Recognition], [光学字符识别],
    [LR], [Low Resolution], [低分辨率],
    [HR], [High Resolution], [高分辨率],
    [SR], [Super-Resolution], [超分辨率],
    hlinex()
  ),
  kind: table,
  caption: [中英术语对照表]
)

= 绪论

== 课题背景和意义

从一般性的场景图片中识别文本信息不仅能帮助深度学习模型在训练时理解场景逻辑，还能在推理时给予使用者更多场景相关的信息，该类型的任务被称为场景文本识别（Scene-Text Recognition, STR）。
场景文本识别能在诸多实际应用中得到重要应用：例如在辅助驾驶中，汽车可以动态识别路上的交通符号，以便根据路况和交通规则做出安全的决策。
// TODO: 多个引用只显示首尾两个
近年来有许多研究人员躬身场景文本识别领域并取得了很多优秀成果。这些模型在目前广泛使用的基准数据集 ~@cute80~@icdar2013~@coco-text 中对于高分辨率（High-Resolution, HR）的图像取得了非常高的准确率。
现实中的STR任务比基准数据集的形式更为复杂，被识别的字形可能因设计要求等因素，单个字符可以有多种字体和不同程度的扭曲，因此也有很多研究人员提出可以使用矫正模块 @aster 对场景文本进行空间上的矫正，使其便于识别。因此给一张比较清晰的场景文本图像，当前有许多方式可以以较高置信度对其进行识别。

然而，在场景文本图像中的特征受到大量影响时，识别器的性能也大幅下降。例如，受到拍摄环境或拍摄器材的制约时，拍摄的场景文本图像可能带有一定的模糊，这对识别器的鲁棒性带来了巨大的挑战。
因此有研究人员 @tsrn 提出了更加一般的STR任务，即对于带有模糊等不良因素的低分辨率（Low-Resolution, LR）图像，可以先进行场景文本图像超分辨率（Scene-Text Image Super-Resolution, STISR）处理，得到较为清晰的超分辨率（Super-Resolution, SR）图像后，再进行常规的STR。该研究人员构建了专门的模糊场景文本数据集 @tsrn 用于评判模型在不良条件下的超分辨率和识别能力。与传统的图像超分辨率（Image Super-Resolution）任务不同，STISR任务更加注重于文本的恢复，由于不少文本包含一定的语义信息，这不仅给图像超分辨率模型带来了挑战，也使STISR模型得到更多新的思路。

对于LR场景文本的识别，当前广泛使用的模型架构是如@fig1-1a 所示的串行架构。这种架构虽然直观，但同时存在不少问题。串行架构中，两个任务的耦合程度太高，STR任务非常依赖于STISR任务的性能，如果STISR得到的SR场景文本图像存在一定的问题，则会对识别任务造成很大的麻烦；同时，STISR任务模型的监督信号含有一定的识别损失而不是专门的超分辨率损失，因此并不会生成比较完整的SR场景文本图像。而本文实现的模型是如@fig1-1b 所示的并行架构，该架构首先使用统一的编码器将输入的图片映射到一个统一的特征空间，再通过不同任务的解码器输出相应的预测目标。该架构的优势在于多种任务使用之间的耦合度低，识别效果受到SR质量影响的程度较低，同时SR的质量也不受识别损失的干扰。

#figure(
  grid(
    columns: 2,
    gutter: 5%,
    [#subfigure(
      image("fig/fig1-1a.svg"),
      caption: [STISR和STR模型的串行架构]
    )<fig1-1a>],
    [#subfigure(
      image("fig/fig1-1b.svg"),
      caption: [STISR和STR模型的并行架构]
    )<fig1-1b>],
  ),
  caption: [两种STISR和STR结合的架构]
)<fig1-1>

== 本文研究内容

本文主要使用如@fig1-1b 所示的多任务模型架构，分别设计了特征编码器、特征增强模块、识别解码器和超分辨率解码器四个部分，并使用了两阶段训练和多任务损失函数进行模型监督训练，最终实现了从LR场景文本图像到文本内容和SR图像的端到端任务。本文的主要贡献如下：

+ 提出了一般性的LR场景文本图像识别和超分辨率的模型架构；
+ 本文的模型使用了基于隐式扩散模型的特征增强模块，并在推理时对扩散模型进行有效的控制，实现在特征层面上增强LR图像；
+ 模型采用两阶段的训练模型，在第一阶段进行预训练，目的是让模型保存高分辨率特征信息，在第二阶段进行特征增强，目的是提升LR图像特征信息，提升多任务模型的泛化能力；
+ 本文使用基于不确定性权重的损失函数进行多任务模型训练，实现STR任务和STISR任务之间的平衡；

== 论文各章节安排

本文主要讨论STR任务和STISR任务的多任务模型和特征增强方法，以及本文所实现的模型在数据集上的性能。
第二章介绍了目前现有的场景文本识别模型和场景文本图像超分辨率模型以及用于特征增强的扩散模型。
第三章将会着重介绍本文提出的模型架构、模型设计思路的实现方式，并会从各个模块出发，分别介绍该模型实现特征增强的方式和多任务解码方式。
第四章主要评估第三章中模型的性能，并展示了多种对比结果。
最后，第五章将会对本文提出的方法进行讨论和总结。

= 相关工作

本章将分别介绍场景文本识别（STR）模型、场景文本图像超分辨率（STISR）模型和隐式扩散模型（Stable Diffusion Model），并讨论其与本文模型的关系。在本章节最后几节，将介绍本工作将要使用的数据集、损失函数和衡量指标等，以便后文能顺利地引入本文模型。

== 场景文本识别

// * NOTES: cnn/rnn-based, attention-based, large model-based
// 1. cnn/rnn: crnn, aster,
// 2. attention: most popular, parseq, svtr, CCD, , SIGA, DPAN, MGP
// 3. large model: ABINet, CLIP4STR, DTrOCR, MATRN
// ! TODO: add formulas
与传统的光学字符识别（OCR）不同，场景文本识别（STR）主要针对场景中的字符进行识别，OCR面向的字符常常比较规整，而STR的识别对象经常由于拍摄问题，出现不同程度的透视或光影效果，有的甚至因为聚焦问题而出现低分辨率（LR）图像。对此问题，目前有很多研究人员提出了许多有效的方法，对高分辨率的场景文本进行识别，有的模型还通过额外设计的模块增强图片中的文本特征。基于不同架构，本节从基于卷积和循环网络、基于注意力机制和基于大语言模型三个角度介绍目前常用的STR模型，以及他们在基准数据集上的性能。

*卷积和循环网络*。卷积和循环网络在计算时拥有绝佳的性能，用该模块设计的网络常常具有训练和推理速度快、模型体积小等特点，因此仍有大量研究人员使用其作为图片特征提取和文本信息处理的主要模块。
例如SVTR~@svtr 模型使用不同步长的卷积运算模拟ViT~@vit 将图片分割为多个小块（patch）的操作，再对其进行后续处理，实验证明，这种图片处理方式能较为高效地将图片嵌入模型。CRNN~@crnn 模型则使用多个卷积层深度提取图像特征，并使用双向的LSTM~@lstm 作为图片和文本之间的映射模块，最终使用CTC损失~@ctc 监督模型训练，其识别结果在当时取得了较高性能。而对于含有透视、扭曲结构等场景文本中，ASTER~@aster 模型在其主干网络之前添加了矫正模块，将弯曲文本矫正为水平排布的文本，并最终使用基于循环网络LSTM~@lstm 和GRU~@gru 的识别模块，最终在弯曲文本的识别中取得非常好的性能。
目前，大部分的模型都使用卷积提取图片特征，这种提取方式的优势在于能够尽量保存原有的图像特征，对于文本识别有很大帮助。本文实现的模型同样使用卷积作为编码器的主干。

*注意力网络*。注意力机制在NLP领域中得到了广泛应用，使用注意力机制能大幅提升模型对较长序列处理能力，不会出现类似LSTM等RNN模型在使用较长序列进行训练时出现的梯度消失或梯度爆炸的现象。对于STR任务，存在两种使用注意力机制的情况，一是针对文本进行注意力学习，再与图像特征进行交叉注意力学习，二是先将图像的特征提取后嵌入模型，再直接对图像进行注意力学习，最后用损失约束输出结果。
例如PARSeq~@parseq 使用了多种注意力机制进行训练。该模型的创新点在于使用排列数作为注意力机制的掩码，在文本特征层面使用了自注意力机制，并与图像特征共同使用交叉注意力，学习文本特征和图像特征中的映射关系，在当时的数据集上取得了较高性能。由于PARSeq模型在多个合成数据集上进行了大规模预训练，因此最终得到的网络具有很强的先验知识，可以根据预训练网络进行微调。
前文提到SVTR~@svtr 模型利用带有不同步长的卷积运算模拟ViT将图像嵌入网络，同时，该模型还使用多个注意力机制用于提取局部和全局特征，该注意力机制使用不同大小的掩码块引导模型对图像注意力的学习。
除此之外，MGP-STR~@mgp-str 以ViT为基础架构，将输入的文本图像分割为互不重叠小块。相较于ViT将不同的图像小块（即每个块的特征）聚合在一起作为整个图像的特征表示，在图像文本识别钟，MGP-STR使用了一种基于注意力机制模块对识别器进行多重约束，用于提取更加注重于文本的特征，将图像的小块有意义地聚合在一起，使得该模型可以预测字母、词组和字母个数等不同粒度的文本信息。
DPAN~@dpan 模型在架构上进行了研究，并基于并行解耦的编码器-解码器架构（Parallel-Decoupled Encoder Decoder, PDED），改进了其注意力的查询输入矩阵（query），从而弥补了查询矩阵和键值矩阵（key）之间的图像信息，提高了模型的鲁棒性。
尽管注意力机制的特征表示能力比卷积和循环模块强，但注意力机制可以让模型在全局特征的角度上进行计算，适合长度适中的特征序列，但其计算复杂度随着序列长度的增加而成平方及增长，不仅如此，其模型参数规模也远大于卷积和循环模块。

*大语言模型辅助网络*。在大语言模型（Large-Language Model, LLM）兴起后，很多与文本相关的任务都可以借助大语言模型实现。由于大语言模型使用大量的数据集进行训练，而模型也拥有巨大的参数量，因此训练出来的大语言模型有着很强的特征表示能力。早期的大语言模型如BERT~@bert 基于Transformer~@transformer 编码器设计了双向编码的无监督训练，其参数量达到了1.1亿（110 million~@bert ）。后续的GPT系列~@gpt 模型则使用自监督的方式进行训练，其中GPT-3~@gpt3 的参数量达到了1.75千亿（175 billion~@gpt3）。在研究图像和文本之间的关系中，CLIP~@clip 多模态模型使用对比学习，同时得到了文本和图像的特征表示编码器。大语言模型中拥有的大量参数使其具有很强的特征的表示能力，因此目前有研究人员直接使用大语言模型的编码器对下游任务中的数据集进行编码表示，并设计模型直接学习编码后的特征。
使用预训练的大模型可以在文本识别任务中取得较好性能。例如在CLIP4STR~@clip4str 工作中借助了视觉语言模型（Vision-Language Model, VLM），认为当前大部分识别器是基于单模态进行训练的，而VLM给予了文本和对应图像的信息，该模型通过视觉解码器和混合模态解码器的预测结果优化，在11个基准测试集中达到了较高的识别准确率。
与此同时，由于大语言模型含有丰富的上下文语义信息，因此有研究人员使用大模型进行文本预测结果的优化。例如ABINet~@abinet 设计了两个分支处理场景文本识别，其一是以残差网络和Transformer为基础架构，并且带有位置注意力的视觉分支，其二则是以CLIP为基础的，用于优化预测文本的语言模型分支。实验发现使用语言模型进行结果的迭代优化可以在基准数据集上达到较高识别准确率。
正如前文所述，使用预训练的大语言模型作为特征编码器或预测优化器，可以达到较高的识别准确率，同时由于大语言模型的强语义性，识别的结果绝大多数依然带有语义性，即识别结果是正确的单词。但在场景文本中，存在许多无上下文语义的文本，因此在使用大语言模型时需要权衡语义信息和图像信息之间的分配。

== 场景文本图像超分辨率

// * NOTES: interpolation based, attention based, diffusion based
// 0. introduce pixel shuffle module
// 1.interpolation based: bilinear and bicubic
// 2.attention based: TATT, DPMN,
// 3.diffusion based: SR3

场景文本图像超分辨率（STISR）的目的是提高低分辨率文本图像的质量，STISR能大大提升前文提到的STR任务对低分辨率文本的识别率，因此对该领域的研究对自动驾驶等下游任务的发展有着重要意义。STISR需要弥补图像特征和文本语义信息之间的模态差异，与单张图像的超分辨率略有不同，STISR的工作包含了文本的语义信息，在增强图像分辨率的同时不能破坏文本结构，因此STISR的约束条件比普通单张图像超分辨率更强。目前，已经有很多研究人员在该领域做出了巨大贡献。本节先介绍传统插值的图像超分辨率方法，接着以该方法为基础，引出当前该领域的相关工作。

#figure(
  image("fig/fig2-2-1.svg", width: 75%),
  placement: auto,
  caption: [（a）表示原始高分辨率图像 $(2h,2w)$，（b）表示压缩后等比放大的低分辨率图像 $(2h,2w)$，（c）表示使用最近邻插值方式的超分图像 $(2h,2w)$，（d）表示使用双二次插值的超分图像 $(2h,2w)$，（e）表示使用双三次插值的超分图像 $(2h,2w)$]
)<fig2-2-1>

*插值*。插值主要目的是根据给定数据点，估计给定点附近的值。用于图像超分辨率的插值方式包括最近邻插值、双线性插值和双三次插值等方式。如@fig2-2-1 假设原始图像大小为 $(h, w)$，以将图片分辨率放大到原来的*两倍*为例，即超分辨后的图像大小为 $(2h,2w)$，分别讨论三种超分辨率方式的效果。对于一个像素点，最近邻插值忽略其周围像素值，直接在周围扩展该像素的值，其效果如@fig2-2-1 （c）所示，可以发现其锯齿化比较严重。而双线性插值会考虑到以该像素点为顶点的周围像素值，假设四个顶点分别为 $A(x_1,y_1),B(x_1,y_2),C(x_2,y_2),D(x_2,y_1)$，这些点所代表的像素值分别由 $f(A),f(B),f(C),f(D)$ 给出，则该四个顶点间任意一个点的像素值 $P(x,y)$ 由公式@eq2-2-1 @eq2-2-2 和@eq2-2-3 给出。其插值效果如@fig2-2-1 （d）所示。相较于最近邻插值，双线性插值在一定程度上减轻了锯齿化。而双三次插值插值一次使用的像素点个数是双线性插值的 $4$ 倍，即使用 $16$ 个像素值估计一个像素值，类似于双线性插值其基本思想是使用三次多项式函数拟合给定的四个像素值，并通过一阶导数和二阶混合偏导保证连续性，从而得到 $16$ 个方程组成的方程组，解出 $16$ 个顶点对应的权重。双三次插值的效果如@fig2-2-1 （e）所示，可以看出其效果比双线性插值多了更多的图像细节。

$
f(P_y_1)=(x_2-x)/(x_2-x_1) f(A) + (x-x_1)/(x_2-x_1) f(B)
$ <eq2-2-1>

$
f(P_y_2)=(x_2-x)/(x_2-x_1) f(D) + (x-x_1)/(x_2-x_1) f(C)
$ <eq2-2-2>

$
f(P)=(x_2-x)/(x_2-x_1) f(P_y_1) + (x-x_1)/(x_2-x_1) f(P_y_2)
$ <eq2-2-3>

使用插值的方式进行图像超分辨率，是在图像的宽和高两个维度上进行像素扩展，在使用深度学习模型进行超分辨率时，需要先将LR图像插值到SR图像，再使用卷积、注意力机制等模块增强其分辨率。而近年来提出的像素重排（PixelShuffle）模块~@pixelshuffle 可以直接在LR图像上提取特征，它的主要思想是将通道维度特征重组到宽和高维度上，该方式既能提高图像分辨率，又能在通道维度上提取特征，是当前网络增强图像分辨率的主要方式。如@fig2-2-2 所示，像素重排主要基于网络提取的特征图，特征图的通道维度上是具有超分辨像素信息的特征，通过重排后，通道维度的像素信息补全到宽和高两个维度，从而增强图像分辨率。与传统的插值方式相比，这种在通道维度上提取特征的方式可以减少网络传播中特征图的大小，同时也更适合具有卷积模块的网络在通道层面上提取特征的方式。基于该模块，有很多研究人员提出了新的超分辨率模型，并在基准数据集上取得了较好的结果。

#figure(
  image("fig/fig2-2-2.svg", width: 75%),
  placement: auto,
  caption: [像素重排（PixelShuffle）模块的主要操作原理。原特征图的通道维度被重排至宽和高两个维度，从而提高图像的分辨率]
)<fig2-2-2>

*卷积和循环网络*。使用卷积和循环网络搭建模型的代表性工作是TSRN~@tsrn，同时该模型的作者也是STISR任务的提出者，并创建了新的数据集用于训练和测试。该模型的主要思路是残差学习，模型使用了重复的超分子模块SRB~@tsrn 对超分图像的残差进行学习后，将残差与浅层特征进行加和，最后使用像素重排输出较高分辨率的图像。在整个网络的特征传播中，特征图的大小保持一致且与LR图像大小相同，模型仅在通道维度上进行特征学习，与前文介绍的像素重排相互印证。

*注意力网络*。前文提到的TSRN是STISR领域的开山之作，因而有研究人员延续其特点，提出了新的架构。例如TATT模型~@tatt 在此基础上增加了注意力机制，目的是混合先验文本信息和图像特征，在一定程度上用文本的先验特征引导超分辨率的进行。TATT模型在LR图像输入时，通过预训练识别器对LR图像先进行一次识别得到预测的文本标签，同时用多个卷积模块提取LR图像的图像特征，再将二者输入到带有交叉注意力的特征融合模块，最后通过TSRN的主干网络输出超分辨率图像。与此思想类似的工作还包括TPGSR~@tpgsr ，该模型同样使用了文本先验模块对超分辨率模块进行引导，并同时使用由高分辨率图像产生的文本先验约束LR图像的文本先验，以便生成更可靠的文本先验。由于大部分超分网络具有一定的缺陷，有研究人员基于这些超分网络，设计了即插即用的超分辨率模块用于提高预训练超分模型的性能。DPMN~@dpmn 

*扩散网络*。

== 扩散模型


== 多任务损失函数


== 数据集


= 模型原理与设计


== 模型总体架构


== 编码器模块设计


== 特征增强模块设计


== 识别器模块设计


== 超分辨模块设计


= 实验结果与分析

== 模型总体性能

== 与目前工作的对比

= 总结与展望

#seu_bibliography("../ref.bib")

#appendix[
  = 扩散模型推导
  = 模型推理结果
]

#acknowledgment[]